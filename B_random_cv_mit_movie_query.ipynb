{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "curr_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sets_filepath = os.path.join(curr_dir,'training_set','movie_queries_training_dataset.csv').replace('\\\\','/')\n",
    "word_vectors_filepath = os.path.join(curr_dir,'word_vector','word_vector.txt').replace('\\\\','/')\n",
    "target_to_index_filepath = os.path.join(curr_dir,'index_converter','target_to_index.txt').replace('\\\\','/')\n",
    "random_search_hist_filepath = os.path.join(curr_dir,'random_search_data','random_search_hist.txt').replace('\\\\','/')\n",
    "random_search_hyperparams_filepath = os.path.join(curr_dir,'random_search_data','random_search_hyperparams.txt').replace('\\\\','/')\n",
    "best_hyperparams_info_filepath = os.path.join(curr_dir,'random_search_data','best_hyperparameter_info.txt').replace('\\\\','/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.data import load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re \n",
    "from keras import backend as k\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, LSTM, Input, concatenate, TimeDistributed, Bidirectional, Masking\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy, crf_accuracy\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras.optimizers import Adam  \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, ParameterSampler, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You only need to run the cell below once, you can delete the cell below and across all notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary to convert categories to index\n",
    "with open(target_to_index_filepath, \"rb\") as t:\n",
    "    target_to_index = pickle.load(t)\n",
    "    \n",
    "f1_labels = list(target_to_index.values())\n",
    "f1_labels.pop(-1)\n",
    "\n",
    "#input_sequence for sentences, output_sequence for targets of sentences\n",
    "input_sequence = []\n",
    "output_sequence = []\n",
    "\n",
    "#Store grid search results\n",
    "random_search_hist = {}\n",
    "\n",
    "#Define function to create base model dynamically\n",
    "#I used a dictionary and formatting to add hidden layers dynamically\n",
    "def base_model(units=50, optimizer='Adam', hidden_layers=2, activation_td ='relu', dropout=0.1, recurrent_dropout=0.1):\n",
    "    hidden_layers_stored = {}\n",
    "    counter=1\n",
    "    input = Input(shape=(x.shape[1],x.shape[-1]))\n",
    "    mask = Masking(mask_value=0.)(input)\n",
    "    for hl in range(hidden_layers):\n",
    "        if counter==1:\n",
    "            hidden_layers_stored['hl_{}'.format(counter)] = Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))(mask)  \n",
    "        else:\n",
    "            hidden_layers_stored['hl_{}'.format(counter)] = Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))(hidden_layers_stored['hl_{}'.format(counter-1)])\n",
    "        counter+=1\n",
    "    model_last_layer = TimeDistributed(Dense(50, activation=activation_td))(hidden_layers_stored['hl_{}'.format(counter-1)])  \n",
    "    crf = CRF(25)  \n",
    "    out = crf(model_last_layer)  \n",
    "    model_final = Model(input, out)\n",
    "    model_final.compile(optimizer=optimizer, loss=crf_loss, metrics=[crf_accuracy])\n",
    "    return model_final\n",
    "\n",
    "#Initialize the hyperparameter number for random cv\n",
    "hyperparam_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all possible pos tags\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "all_pos = list(tagdict.keys())\n",
    "\n",
    "all_pos_tags = []\n",
    "for pos in all_pos:\n",
    "    all_pos_tags.append('pos_'+pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read pre-processed dataset for training\n",
    "df = pd.read_csv(training_sets_filepath)\n",
    "df_target = df.copy()\n",
    "\n",
    "#Get list of words from dataframe\n",
    "tokenized_text = df['word'].tolist()\n",
    "\n",
    "#Call Word Vectorization API and load the processed word vectors\n",
    "word_vector_api_data = tokenized_text\n",
    "session = requests.Session()\n",
    "session.trust_env = False\n",
    "session.post('http://127.0.0.1:5000/word_vectorization', json = word_vector_api_data) #add proxies args if needed\n",
    "\n",
    "with open(word_vectors_filepath, \"rb\") as t:\n",
    "    word_vectors = pickle.load(t)\n",
    "\n",
    "#Add word featues to dataframe\n",
    "df['word_vec'] = word_vectors\n",
    "df = pd.get_dummies(df, columns=['pos'])\n",
    "\n",
    "#Add missing pos columns \n",
    "df_cols = list(df.columns)\n",
    "add_pos_col = [add for add in all_pos_tags if add not in df_cols]\n",
    "\n",
    "#Assign a binary value of 0 to the newly added pos columns as they are not present in our training data\n",
    "for added_pos in add_pos_col:\n",
    "    df[added_pos] = 0\n",
    "\n",
    "#Rearrange columns in fixed order for consistency\n",
    "arrange_df_cols = ['sentence_no','word','word_vec']\n",
    "for arrange_pos in all_pos_tags:\n",
    "    arrange_df_cols.append(arrange_pos)\n",
    "df = df.reindex(columns=arrange_df_cols)\n",
    "\n",
    "#Get the sentence feature vectors. Each sentence contains a list of all its word feature vectors.\n",
    "df = df.drop(columns=['word'])\n",
    "sentence_feature_vectors = {}\n",
    "for index,row in df.iterrows():\n",
    "    sentence_number = row[0]\n",
    "    word_feature_vector = np.concatenate((row[1:]), axis = None)\n",
    "    if sentence_number in sentence_feature_vectors.keys():\n",
    "        sentence_feature_vectors[sentence_number].append(word_feature_vector)\n",
    "    else:\n",
    "        sentence_feature_vectors[sentence_number] = [word_feature_vector]\n",
    "\n",
    "#Pad length for sentences and append to the input_sequence \n",
    "dummy_length = len(sentence_feature_vectors[1][0])\n",
    "for sentence in sentence_feature_vectors.values():\n",
    "    while len(sentence) < 80:\n",
    "        sentence.append(np.array([0 for zero in range(dummy_length)]))\n",
    "\n",
    "    input_sequence.append(np.array(sentence))\n",
    "\n",
    "#Add the target for each word of the sentence\n",
    "#Each key is a sentence number and each value is the targets of that sentence in this dictionary\n",
    "targets = {}\n",
    "for index,row in df_target.iterrows():\n",
    "    sentence_number = row[1]\n",
    "    word_target = row[-1]\n",
    "    if sentence_number in targets.keys():\n",
    "        targets[sentence_number].append(word_target)\n",
    "    else:\n",
    "        targets[sentence_number] = [word_target]\n",
    "\n",
    "#Convert the targets to their respective index, pad length for sentences and append to output_sequence\n",
    "for sentence in targets.values():\n",
    "    sentence = [target_to_index[target] for target in sentence]\n",
    "    while len(sentence) < 80:\n",
    "        sentence.append(target_to_index['O'])\n",
    "\n",
    "    output_sequence.append(np.array(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(input_sequence)\n",
    "y = np.array(output_sequence)\n",
    "#CRF needs the input and output sequence to be 3 Dimensional, which is why one-hot encoding is done for the y output values\n",
    "y = to_categorical(y, num_classes=25)\n",
    "x_s,y_s = shuffle(x,y,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue random cv\n",
    "\n",
    "1. Load the whole list of parameters and the random search hist\n",
    "2. Remove all the parameters that have been tried\n",
    "3. Set the parameter number to continue from where we left off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(random_search_hyperparams_filepath, \"rb\") as t:\n",
    "    hyperparams_list = pickle.load(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(random_search_hist_filepath, \"rb\") as t:\n",
    "    random_search_hist = pickle.load(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(random_search_hist)):\n",
    "    hyperparams_list.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_number = len(random_search_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search Cross Validation (CV)\n",
    "\n",
    "Find the best combination of hyperparameters for the algorithm in order for it to best learn from our use case.\n",
    "\n",
    "The random search finds sets of random combination of hyperparameters for the model to iteratively test out. \n",
    "\n",
    "Cross validation is then used to evaluate the performance of the model. Cross validation splits the data into an arbitrary number of sets. For instance, it splits the data into 5 sets. On the first run it will be trained on set 1 to 4 and tested on 5. Next, it will be trained on set 2 to 5 and tested on 1, so on and so forth. \n",
    "\n",
    "This is effective in validating performance as it is tested on multiple sets of unseen data. If the model performs well during cross validation, chances are that it has learnt patterns that generalize well to our use case as it can predict unseen data well.\n",
    "\n",
    "**Random Search vs Grid Search**\n",
    "\n",
    "* Random Search is more feasible as grid search will run over **every** parameter aka, 1800 parameters! This will take up a very long time and thus, unfeasible. \n",
    "* Proven that it has a 95% probability of finding a combination of hyperparameters within the top 5% best performing combinations using 60 iterations.\n",
    "\n",
    "Libraries such as keras and sklearn do not provide random search cv for 3 dimensional inputs and outputs so custom random search cv code needs to be written.\n",
    "\n",
    "1. Generate a random set (45 sets) of parameters using sklearn's ParameterSampler class\n",
    "\n",
    "2. Manually split the data into 5 folds using subsetting \n",
    "\n",
    "3. Write code to train on 4 folds of training data and test on the last one, repeating this process 5 times where the test set is a new fold every iteration and the rest of the folds are the training data. The performance metric/result of the test set, f1 score, is saved to a list at every iteration. At the end of the iteration, the evaluated set of parameter, the average of the f1 scores and all the f1 scores are added to the dictionary\n",
    "\n",
    "*a condition is put to stop the cross validation for a set of parameter if its f1 score is below 0.7 after 2 iterations to save time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a measure of performance\n",
    "\n",
    "F1 Score Vs Accuracy\n",
    "\n",
    "Accuracy is a misleading indicator for imbalanced datasets. In our case 'O' entities makes up majority of all entities\n",
    "\n",
    "Example:\n",
    "\n",
    "* Positive class = Location\n",
    "* Negative class = Non Location\n",
    "\n",
    "\n",
    "Accuracy: (Number of Correct Predictions) / (Total Number of Predictions), (1 + 90)/(100), 91% \n",
    "\n",
    "We aim to predict location entities, however out of the 9 location entities we only predicted 1 correctly. Thus accuracy creates the illusion that our model performs very well by taking the correct predictions of the majority negative class into account.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Precision: (Number of Correct Positive Predictions) / (Total Number of Positive Predictions), (1)/(1+1), 50%\n",
    "\n",
    "Preicison measures the proportion of positive predictions made that are correct. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Recall: (Number of Correct Positive Predictions) / (Total Number of Positives), (1)/(1+8), 11%\n",
    "\n",
    "Recall measures the proportion of actual positives that were captured\n",
    "\n",
    "<br>\n",
    "\n",
    "F1 score is a combination of precision and recall, and is a much better reflection of a model's performance for predicting the positive class, which is the class we are aiming to predict\n",
    "\n",
    "The f1 macro score is used as it calculates the f1 score for each label and averages them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_hyperparams = [16, 32, 64, 128, 256]\n",
    "epochs_hyperparams = [30,50,80,100,150,200,250]\n",
    "units_hyperparams = [50, 100, 150, 200]\n",
    "optimizer_hyperparams = ['RMSprop', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "hidden_layers_hyperparams = [1, 2, 3]\n",
    "dropout_hyperparams = [0.1,0.2,0.3]\n",
    "recurrent_dropout_hyperparams = [0.1,0.2,0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random set of hyperparameters to generate from:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_size_hyperparams': [16, 32, 64, 128, 256],\n",
       " 'epochs_hyperparams': [30, 50, 80, 100, 150, 200, 250],\n",
       " 'units_hyperparams': [50, 100, 150, 200],\n",
       " 'optimizer_hyperparams': ['RMSprop', 'Adadelta', 'Adam', 'Adamax', 'Nadam'],\n",
       " 'hidden_layers_hyperparams': [1, 2, 3],\n",
       " 'dropout_hyperparams': [0.1, 0.2, 0.3],\n",
       " 'recurrent_dropout_hyperparams': [0.1, 0.2, 0.3]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams_grid = dict(batch_size_hyperparams=batch_size_hyperparams, epochs_hyperparams=epochs_hyperparams, units_hyperparams=units_hyperparams, optimizer_hyperparams=optimizer_hyperparams, hidden_layers_hyperparams = hidden_layers_hyperparams, dropout_hyperparams=dropout_hyperparams, recurrent_dropout_hyperparams=recurrent_dropout_hyperparams)\n",
    "print(\"Random set of hyperparameters to generate from:\")\n",
    "hyperparams_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the 60 random sets of combinations of hyperparameters\n",
    "random_hyperparams = ParameterSampler(hyperparams_grid, n_iter=60) \n",
    "hyperparams_list = list(random_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'units_hyperparams': 150,\n",
       " 'recurrent_dropout_hyperparams': 0.3,\n",
       " 'optimizer_hyperparams': 'Adam',\n",
       " 'hidden_layers_hyperparams': 2,\n",
       " 'epochs_hyperparams': 30,\n",
       " 'dropout_hyperparams': 0.1,\n",
       " 'batch_size_hyperparams': 256}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following combination of parameters is found to work quite well, which is why I added it in. You can remove it if you want to start afresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'units_hyperparams': 100, 'recurrent_dropout_hyperparams': 0.3, 'optimizer_hyperparams': 'Adadelta', 'hidden_layers_hyperparams': 1, 'epochs_hyperparams': 250, 'dropout_hyperparams': 0.2, 'batch_size_hyperparams': 32} in hyperparams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_list[0]={'units_hyperparams': 100, 'recurrent_dropout_hyperparams': 0.3, 'optimizer_hyperparams': 'Adadelta', 'hidden_layers_hyperparams': 1, 'epochs_hyperparams': 250, 'dropout_hyperparams': 0.2, 'batch_size_hyperparams': 32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random set of hyperparameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'units_hyperparams': 100,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'Adadelta',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 250,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 32},\n",
       " {'units_hyperparams': 150,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Adam',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 100,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 256},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'RMSprop',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 80,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 32},\n",
       " {'units_hyperparams': 150,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Adamax',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 250,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 32},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'Adamax',\n",
       "  'hidden_layers_hyperparams': 3,\n",
       "  'epochs_hyperparams': 250,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 16},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'Adamax',\n",
       "  'hidden_layers_hyperparams': 3,\n",
       "  'epochs_hyperparams': 150,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 256},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Adamax',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 150,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 32},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Adam',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 150,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 32},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Adamax',\n",
       "  'hidden_layers_hyperparams': 3,\n",
       "  'epochs_hyperparams': 250,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 256},\n",
       " {'units_hyperparams': 150,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Adamax',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 200,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 256},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'RMSprop',\n",
       "  'hidden_layers_hyperparams': 3,\n",
       "  'epochs_hyperparams': 80,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 16},\n",
       " {'units_hyperparams': 150,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Nadam',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 200,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 16},\n",
       " {'units_hyperparams': 100,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Adadelta',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 80,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 16},\n",
       " {'units_hyperparams': 100,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'RMSprop',\n",
       "  'hidden_layers_hyperparams': 3,\n",
       "  'epochs_hyperparams': 150,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 32},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'Nadam',\n",
       "  'hidden_layers_hyperparams': 3,\n",
       "  'epochs_hyperparams': 150,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 64},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'Adamax',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 150,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 32},\n",
       " {'units_hyperparams': 150,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Adam',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 30,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 128},\n",
       " {'units_hyperparams': 150,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Adamax',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 30,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 16},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'RMSprop',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 80,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 128},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'RMSprop',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 80,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 128},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Nadam',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 150,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 32},\n",
       " {'units_hyperparams': 150,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Nadam',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 150,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 128},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'RMSprop',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 50,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 16},\n",
       " {'units_hyperparams': 150,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'Adadelta',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 80,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 64},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Adadelta',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 150,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 256},\n",
       " {'units_hyperparams': 100,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'Nadam',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 30,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 128},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Adam',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 150,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 128},\n",
       " {'units_hyperparams': 150,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Adam',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 50,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 16},\n",
       " {'units_hyperparams': 100,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Adamax',\n",
       "  'hidden_layers_hyperparams': 3,\n",
       "  'epochs_hyperparams': 80,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 128},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Nadam',\n",
       "  'hidden_layers_hyperparams': 3,\n",
       "  'epochs_hyperparams': 50,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 128},\n",
       " {'units_hyperparams': 150,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'RMSprop',\n",
       "  'hidden_layers_hyperparams': 3,\n",
       "  'epochs_hyperparams': 100,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 128},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Adamax',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 30,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 64},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Nadam',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 250,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 128},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'RMSprop',\n",
       "  'hidden_layers_hyperparams': 3,\n",
       "  'epochs_hyperparams': 30,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 64},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Adamax',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 80,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 128},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Nadam',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 30,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 32},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'RMSprop',\n",
       "  'hidden_layers_hyperparams': 3,\n",
       "  'epochs_hyperparams': 50,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 16},\n",
       " {'units_hyperparams': 100,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'Nadam',\n",
       "  'hidden_layers_hyperparams': 3,\n",
       "  'epochs_hyperparams': 50,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 256},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'Adadelta',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 30,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 16},\n",
       " {'units_hyperparams': 100,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Adam',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 30,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 64},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Adadelta',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 150,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 64},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'RMSprop',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 250,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 32},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Nadam',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 50,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 128},\n",
       " {'units_hyperparams': 100,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Adam',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 200,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 16},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Adam',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 150,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 64},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Adadelta',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 100,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 32},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'Adam',\n",
       "  'hidden_layers_hyperparams': 3,\n",
       "  'epochs_hyperparams': 30,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 64},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Adadelta',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 50,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 64},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'Adamax',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 100,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 128},\n",
       " {'units_hyperparams': 100,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'Adamax',\n",
       "  'hidden_layers_hyperparams': 3,\n",
       "  'epochs_hyperparams': 80,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 16},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Adam',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 80,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 64},\n",
       " {'units_hyperparams': 100,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'Nadam',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 50,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 16},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.3,\n",
       "  'optimizer_hyperparams': 'Nadam',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 150,\n",
       "  'dropout_hyperparams': 0.3,\n",
       "  'batch_size_hyperparams': 16},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Adamax',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 50,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 128},\n",
       " {'units_hyperparams': 100,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Adam',\n",
       "  'hidden_layers_hyperparams': 3,\n",
       "  'epochs_hyperparams': 150,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 256},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.1,\n",
       "  'optimizer_hyperparams': 'Adamax',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 50,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 256},\n",
       " {'units_hyperparams': 200,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Nadam',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 150,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 32},\n",
       " {'units_hyperparams': 50,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Adam',\n",
       "  'hidden_layers_hyperparams': 2,\n",
       "  'epochs_hyperparams': 100,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 32},\n",
       " {'units_hyperparams': 150,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Nadam',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 30,\n",
       "  'dropout_hyperparams': 0.1,\n",
       "  'batch_size_hyperparams': 16},\n",
       " {'units_hyperparams': 100,\n",
       "  'recurrent_dropout_hyperparams': 0.2,\n",
       "  'optimizer_hyperparams': 'Adamax',\n",
       "  'hidden_layers_hyperparams': 1,\n",
       "  'epochs_hyperparams': 150,\n",
       "  'dropout_hyperparams': 0.2,\n",
       "  'batch_size_hyperparams': 16}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Random set of hyperparameters:\")\n",
    "hyperparams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(random_search_hyperparams_filepath, \"wb\") as t:\n",
    "    pickle.dump(hyperparams_list, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9775, 80, 95)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9775, 80, 25)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting cross validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = x_s[:1955]\n",
    "x_2 = x_s[1955:3910]\n",
    "x_3 = x_s[3910:5865]\n",
    "x_4 = x_s[5865:7820]\n",
    "x_5 = x_s[7820:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold contains approx 1955 sentences:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1955, 80, 95)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Each fold contains approx 1955 sentences:\")\n",
    "x_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1 = y_s[:1955]\n",
    "y_2 = y_s[1955:3910]\n",
    "y_3 = y_s[3910:5865]\n",
    "y_4 = y_s[5865:7820]\n",
    "y_5 = y_s[7820:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cv = [x_1,x_2,x_3,x_4,x_5]\n",
    "y_cv = [y_1,y_2,y_3,y_4,y_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/250\n",
      "7820/7820 [==============================] - 121s 15ms/step - loss: 28.7445 - crf_accuracy: 0.6912\n",
      "Epoch 2/250\n",
      " 832/7820 [==>...........................] - ETA: 1:24 - loss: 28.7323 - crf_accuracy: 0.7987"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f3fa8bf2609d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[0mmodel_randomcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhyperparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'units_hyperparams'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhyperparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimizer_hyperparams'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhyperparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hidden_layers_hyperparams'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhyperparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dropout_hyperparams'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhyperparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'recurrent_dropout_hyperparams'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;31m#Train the model on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                 \u001b[0mmodel_randomcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtraining_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytraining_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhyperparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epochs_hyperparams'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhyperparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size_hyperparams'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;31m#Calculate f1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for hyperparam in hyperparams_list:\n",
    "    #If this is the first hyperparameter in the random search, don't load the random search history file \n",
    "    #because there won't be a file yet\n",
    "    if hyperparam_number!=0:\n",
    "        with open(random_search_hist_filepath, \"rb\") as t:\n",
    "            random_search_hist = pickle.load(t)\n",
    "            \n",
    "    #Stores information regarding the random cv of the hyperparameter\n",
    "    \n",
    "    temp_list = []\n",
    "    for cv in range(5):\n",
    "        print('#############################')\n",
    "        \n",
    "        #Selecting the x training set i.e. 4 training sets other than the training set at the current index, and\n",
    "        #respective y training set\n",
    "        select_xtraining_set = [train for i,train in enumerate(x_cv) if i!=cv]\n",
    "        xtrain_1 = select_xtraining_set[0]\n",
    "        xtrain_2 = select_xtraining_set[1]\n",
    "        xtrain_3 = select_xtraining_set[2]\n",
    "        xtrain_4 = select_xtraining_set[3]\n",
    "        xset_1 = np.append(xtrain_1,xtrain_2,axis=0)\n",
    "        xset_2 = np.append(xset_1,xtrain_3,axis=0)\n",
    "        xtraining_set = np.append(xset_2,xtrain_4,axis=0)\n",
    "        \n",
    "        select_ytraining_set = [train for i,train in enumerate(y_cv) if i!=cv]\n",
    "        ytrain_1 = select_ytraining_set[0]\n",
    "        ytrain_2 = select_ytraining_set[1]\n",
    "        ytrain_3 = select_ytraining_set[2]\n",
    "        ytrain_4 = select_ytraining_set[3]\n",
    "        yset_1 = np.append(ytrain_1,ytrain_2,axis=0)\n",
    "        yset_2 = np.append(yset_1,ytrain_3,axis=0)\n",
    "        ytraining_set = np.append(yset_2,ytrain_4,axis=0)\n",
    "        \n",
    "        #Selecting the x testing set i.e. the training set at the current index, and\n",
    "        #respective y training set\n",
    "        xtest_set = [test for i,test in enumerate(x_cv) if i==cv][0]\n",
    "        y_true = [test for i,test in enumerate(y_cv) if i==cv][0]\n",
    "        \n",
    "        #GPU Options are added to prevent the program from taking up all the computer GPU's memory.\n",
    "        graph_randomcv = tf.Graph()\n",
    "        with graph_randomcv.as_default():\n",
    "            config = tf.ConfigProto()\n",
    "            config.gpu_options.allow_growth = True\n",
    "            config.log_device_placement = True\n",
    "            session_randomcv = tf.Session(config=config)\n",
    "            with session_randomcv.as_default():\n",
    "                #Initializing the model with the hyperparameters to be evaluated\n",
    "                model_randomcv = base_model(units=hyperparam['units_hyperparams'], optimizer=hyperparam['optimizer_hyperparams'], hidden_layers = hyperparam['hidden_layers_hyperparams'], dropout = hyperparam['dropout_hyperparams'], recurrent_dropout = hyperparam['recurrent_dropout_hyperparams'])\n",
    "                #Train the model on the training data\n",
    "                model_randomcv.fit(xtraining_set, ytraining_set, epochs=hyperparam['epochs_hyperparams'], batch_size=hyperparam['batch_size_hyperparams'])\n",
    "        \n",
    "        #Calculate f1\n",
    "        #Get the prediciton on the test set and reshape both y sets so that it would be 2D \n",
    "        #as sklearn's f1 evaluation only accepts 2D inputs. Just all the words and \n",
    "        #their corresponding targets, not split into sentences.\n",
    "        y_pred = model_randomcv.predict(xtest_set)\n",
    "        yshape_true = y_true.shape\n",
    "        yshape_pred = y_pred.shape\n",
    "        y_true_newshape = (yshape_true[0]*yshape_true[1], yshape_true[-1])\n",
    "        y_pred_newshape = (yshape_pred[0]*yshape_pred[1], yshape_pred[-1])\n",
    "        y_true_reshaped = np.reshape(y_true, y_true_newshape)\n",
    "        y_pred_reshaped = np.reshape(y_pred, y_pred_newshape)\n",
    "        \n",
    "        #release memory so that process can run faster and won't hit memory errors\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        try:\n",
    "            temp_list.append(f1_score(y_true_reshaped, y_pred_reshaped, average = 'macro', labels=f1_labels))\n",
    "            print(param)\n",
    "            print(' ')\n",
    "            print(f1_score(y_true_reshaped, y_pred_reshaped, average = 'macro', labels=f1_labels))\n",
    "            print(' ')\n",
    "        except:\n",
    "            print('Predicted NaN')\n",
    "            print(' ')\n",
    "            temp_list.append(0)\n",
    "            \n",
    "        #On the first cross validation if the score is below 0.6 go to the next hyperparameter\n",
    "        if cv==0 and np.average(temp_list)<0.7:\n",
    "            break\n",
    "\n",
    "    calculated_f1 = np.average(temp_list)\n",
    "    print('F1 score')\n",
    "    print(param)\n",
    "    print(calculated_f1)\n",
    "    print(' ')\n",
    "    random_search_hist[hyperparam_number] = [hyperparam, calculated_f1, temp_list]\n",
    "    \n",
    "    #Dump the random_searh_dict in case of any unforseen circumstances to save progress\n",
    "    with open(random_search_hist_filepath, \"wb\") as t:\n",
    "        pickle.dump(random_search_hist, t)\n",
    "        \n",
    "    hyperparam_number+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameter_info = []\n",
    "for key,value in random_search_hist.items():\n",
    "    final_score = value[1]\n",
    "    if key==0:\n",
    "        best_hyperparameter_info = [final_score,random_search_hist[key][0]]\n",
    "    elif final_score > best_parameter_info[0]:\n",
    "        best_hyperparameter_info = [final_score,random_search_hist[key][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameter_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(best_hyperparams_info_filepath, \"wb\") as t:\n",
    "    pickle.dump(best_hyperparameter_info, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
