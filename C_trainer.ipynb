{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: Do not run the B_random_cv_mit_movie_query notebook at the same time as this notebook, as it is not recommended to have > 1 heavy tensorflow process running at the same time such as training or performing random search cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "curr_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sets_filepath = os.path.join(curr_dir,'training_set','movie_queries_training_dataset.csv').replace('\\\\','/')\n",
    "word_vectors_filepath = os.path.join(curr_dir,'word_vector','word_vector.txt').replace('\\\\','/')\n",
    "target_to_index_filepath = os.path.join(curr_dir,'index_converter','target_to_index.txt').replace('\\\\','/')\n",
    "save_weights_filepath = os.path.join(curr_dir,'model_training_weights','weights.{epoch:02d}.hdf5').replace('\\\\','/')\n",
    "f1_hist_filepath = os.path.join(curr_dir,'training_hist','f1_hist.txt').replace('\\\\','/')\n",
    "best_hyperparams_info_filepath = os.path.join(curr_dir,'random_search_data','best_hyperparameter_info.txt').replace('\\\\','/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.data import load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re \n",
    "from keras import backend as k\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, LSTM, Input, concatenate, TimeDistributed, Bidirectional, Masking\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy, crf_accuracy\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras.optimizers import Adam  \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, TensorBoard\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, ParameterSampler, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You only need to run the cell below once, you can delete the cell below and across all notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dictionary to convert categories to index\n",
    "with open(target_to_index_filepath, \"rb\") as t:\n",
    "    target_to_index = pickle.load(t)\n",
    "    \n",
    "f1_labels = list(target_to_index.values())\n",
    "f1_labels.pop(0)\n",
    "\n",
    "#input_sequence for sentences, output_sequence for targets of sentences\n",
    "input_sequence = []\n",
    "output_sequence = []\n",
    "\n",
    "#Save all model weights and then select the one with the best f1 score afterwards\n",
    "save_weights = ModelCheckpoint(save_weights_filepath, save_best_only=False, save_weights_only=True, monitor='loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all possible pos tags\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "all_pos = list(tagdict.keys())\n",
    "\n",
    "all_pos_tags = []\n",
    "for pos in all_pos:\n",
    "    all_pos_tags.append('pos_'+pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting best hyperparameters of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(best_hyperparams_info_filepath, \"rb\") as t:\n",
    "    best_hyperparameter_info = pickle.load(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading pre-processed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read pre-processed dataset for training\n",
    "df = pd.read_csv(training_sets_filepath)\n",
    "df_target = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract list of tokenized words from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of words from dataframe\n",
    "tokenized_text = df['word'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectorization: FastText\n",
    "\n",
    "Need to represent words as numbers because machine learning models cannot read raw text\n",
    "\n",
    "\n",
    "* High ability to vectorize out-of-vocabulary words\n",
    "  * Some texts may contain words (names, terminologies) that popular pre-trained word vectorization models such as GloVe and Word2Vec cannot vectorize as these words were very probably not included in their training corpus\n",
    "  * FastText performs word embedding using character n-grams or sub words \n",
    "\n",
    "Example: n-gram = 3, the word 'matter' would be broken into <ma, mat, att, tte, ter, er>\n",
    "\n",
    "Used Pre-Trained Model, not enough data to train a FastText model\n",
    "\n",
    "Vectorized **lower text** because there are more lower case n-grams compared to n-grams with upper case letters in the FastText model's wikipedia training corpus\n",
    "\n",
    "Standardised vectors for numbers as numbers have no semantic meaning, they shouldn't have different vectors\n",
    "\n",
    "### Why make an API for word vectorization instead of including it in the notebook?\n",
    "\n",
    "Pre-Trained FastText model is 7GB and loading it takes up alot of time & memory. Loading >1 FastText model will result in a memory error in the second notebook\n",
    "\n",
    "API allows multiple notebooks to access the word vectorization\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "#Check if the word is a number\n",
    "def containsNumbers(check):\n",
    "    return any(char.isdigit() for char in check)\n",
    "\n",
    "\n",
    "@app.route(\"/word_vectorization\", methods=['POST'])\n",
    "def word_vectorization():\n",
    "    #Initialize empty word_vectors list\n",
    "    word_vectors = []\n",
    "    #Retrieve the list of words\n",
    "    tokenized_text_lower = request.json\n",
    "    #Iterate over the list of words\n",
    "    for word in tokenized_text_lower:\n",
    "        #Standardise vector for numbers as they have no semantic meaning\n",
    "        if containsNumbers(word):\n",
    "            word_vector = ft.get_word_vector('<NUMBER>')\n",
    "            word_vectors.append(word_vector)\n",
    "            continue\n",
    "\n",
    "        #Get the word vector for the word\n",
    "        word_vector = ft.get_word_vector(word)\n",
    "\n",
    "        #Append the word vector to the word_vectors list\n",
    "        word_vectors.append(word_vector)\n",
    "\n",
    "    #Dump the word vectors list\n",
    "    with open(word_vectors_filepath, \"wb\") as t:\n",
    "        pickle.dump(word_vectors, t)\n",
    "    return Response(status = 200)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call Word Vectorization API and load the processed word vectors\n",
    "word_vector_api_data = tokenized_text\n",
    "session = requests.Session()\n",
    "session.trust_env = False\n",
    "session.post('http://127.0.0.1:5000/word_vectorization', json = word_vector_api_data) #add proxies args if needed\n",
    "\n",
    "with open(word_vectors_filepath, \"rb\") as t:\n",
    "    word_vectors = pickle.load(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features to the training dataframe\n",
    "\n",
    "For this set of text, there aren't many useful word features to help in learning. For instance, capitalisation cannot be considered for a word feature as the training data found online contains words that are all lower cased already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add word featues to dataframe\n",
    "df['word_vec'] = word_vectors\n",
    "df = pd.get_dummies(df, columns=['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_no</th>\n",
       "      <th>word</th>\n",
       "      <th>target</th>\n",
       "      <th>word_vec</th>\n",
       "      <th>pos_$</th>\n",
       "      <th>pos_CC</th>\n",
       "      <th>pos_CD</th>\n",
       "      <th>pos_DT</th>\n",
       "      <th>pos_EX</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_VB</th>\n",
       "      <th>pos_VBD</th>\n",
       "      <th>pos_VBG</th>\n",
       "      <th>pos_VBN</th>\n",
       "      <th>pos_VBP</th>\n",
       "      <th>pos_VBZ</th>\n",
       "      <th>pos_WDT</th>\n",
       "      <th>pos_WP</th>\n",
       "      <th>pos_WP$</th>\n",
       "      <th>pos_WRB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>what</td>\n",
       "      <td>O</td>\n",
       "      <td>[0.4232092, -0.67043304, -0.28789037, 0.851811...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>movies</td>\n",
       "      <td>O</td>\n",
       "      <td>[0.2133574, -0.056065083, -1.0704498, 0.231683...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>star</td>\n",
       "      <td>O</td>\n",
       "      <td>[0.09002825, 0.3791883, -0.61237025, -0.014841...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bruce</td>\n",
       "      <td>B-ACTOR</td>\n",
       "      <td>[-0.40589693, 0.67957574, -0.7557319, 0.120949...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>willis</td>\n",
       "      <td>I-ACTOR</td>\n",
       "      <td>[-0.2464564, 0.852988, -0.72730404, 0.00421249...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sentence_no    word   target  \\\n",
       "0           0            0    what        O   \n",
       "1           1            0  movies        O   \n",
       "2           2            0    star        O   \n",
       "3           3            0   bruce  B-ACTOR   \n",
       "4           4            0  willis  I-ACTOR   \n",
       "\n",
       "                                            word_vec  pos_$  pos_CC  pos_CD  \\\n",
       "0  [0.4232092, -0.67043304, -0.28789037, 0.851811...      0       0       0   \n",
       "1  [0.2133574, -0.056065083, -1.0704498, 0.231683...      0       0       0   \n",
       "2  [0.09002825, 0.3791883, -0.61237025, -0.014841...      0       0       0   \n",
       "3  [-0.40589693, 0.67957574, -0.7557319, 0.120949...      0       0       0   \n",
       "4  [-0.2464564, 0.852988, -0.72730404, 0.00421249...      0       0       0   \n",
       "\n",
       "   pos_DT  pos_EX  ...  pos_VB  pos_VBD  pos_VBG  pos_VBN  pos_VBP  pos_VBZ  \\\n",
       "0       0       0  ...       0        0        0        0        0        0   \n",
       "1       0       0  ...       0        0        0        0        0        0   \n",
       "2       0       0  ...       0        0        0        0        1        0   \n",
       "3       0       0  ...       0        0        0        0        0        0   \n",
       "4       0       0  ...       0        0        0        0        0        0   \n",
       "\n",
       "   pos_WDT  pos_WP  pos_WP$  pos_WRB  \n",
       "0        0       1        0        0  \n",
       "1        0       0        0        0  \n",
       "2        0       0        0        0  \n",
       "3        0       0        0        0  \n",
       "4        0       0        0        0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix arrangement of columns\n",
    "\n",
    "We need to add all the pos columns from nltk and rearrange them in order for consistency. We need to add all pos columns, incase future words we predict on have a pos that our training text does not contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Missing pos tags: 12\n",
      " \n",
      "Need to add pos columns: ['pos_LS', \"pos_''\", 'pos_UH', 'pos_--', 'pos_:', 'pos_(', 'pos_)', 'pos_.', 'pos_,', 'pos_``', 'pos_SYM', 'pos_POS']\n"
     ]
    }
   ],
   "source": [
    "#Add missing pos columns \n",
    "df_cols = list(df.columns)\n",
    "add_pos_col = [add for add in all_pos_tags if add not in df_cols]\n",
    "print(' ')\n",
    "print('Missing pos tags: {}'.format(len(add_pos_col)))\n",
    "print(' ')\n",
    "print('Need to add pos columns: {}'.format(add_pos_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign a binary value of 0 to the newly added pos columns as they are not present in our training data\n",
    "for added_pos in add_pos_col:\n",
    "    df[added_pos] = 0\n",
    "\n",
    "#Rearrange in fixed order for consistency\n",
    "arrange_df_cols = ['sentence_no','word','word_vec']\n",
    "for arrange_pos in all_pos_tags:\n",
    "    arrange_df_cols.append(arrange_pos)\n",
    "df = df.reindex(columns=arrange_df_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_no</th>\n",
       "      <th>word</th>\n",
       "      <th>word_vec</th>\n",
       "      <th>pos_LS</th>\n",
       "      <th>pos_TO</th>\n",
       "      <th>pos_VBN</th>\n",
       "      <th>pos_''</th>\n",
       "      <th>pos_WP</th>\n",
       "      <th>pos_UH</th>\n",
       "      <th>pos_VBG</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_MD</th>\n",
       "      <th>pos_VB</th>\n",
       "      <th>pos_WRB</th>\n",
       "      <th>pos_NNP</th>\n",
       "      <th>pos_EX</th>\n",
       "      <th>pos_NNS</th>\n",
       "      <th>pos_SYM</th>\n",
       "      <th>pos_CC</th>\n",
       "      <th>pos_CD</th>\n",
       "      <th>pos_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>what</td>\n",
       "      <td>[0.4232092, -0.67043304, -0.28789037, 0.851811...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>movies</td>\n",
       "      <td>[0.2133574, -0.056065083, -1.0704498, 0.231683...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>star</td>\n",
       "      <td>[0.09002825, 0.3791883, -0.61237025, -0.014841...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bruce</td>\n",
       "      <td>[-0.40589693, 0.67957574, -0.7557319, 0.120949...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>willis</td>\n",
       "      <td>[-0.2464564, 0.852988, -0.72730404, 0.00421249...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_no    word                                           word_vec  \\\n",
       "0            0    what  [0.4232092, -0.67043304, -0.28789037, 0.851811...   \n",
       "1            0  movies  [0.2133574, -0.056065083, -1.0704498, 0.231683...   \n",
       "2            0    star  [0.09002825, 0.3791883, -0.61237025, -0.014841...   \n",
       "3            0   bruce  [-0.40589693, 0.67957574, -0.7557319, 0.120949...   \n",
       "4            0  willis  [-0.2464564, 0.852988, -0.72730404, 0.00421249...   \n",
       "\n",
       "   pos_LS  pos_TO  pos_VBN  pos_''  pos_WP  pos_UH  pos_VBG  ...  pos_MD  \\\n",
       "0       0       0        0       0       1       0        0  ...       0   \n",
       "1       0       0        0       0       0       0        0  ...       0   \n",
       "2       0       0        0       0       0       0        0  ...       0   \n",
       "3       0       0        0       0       0       0        0  ...       0   \n",
       "4       0       0        0       0       0       0        0  ...       0   \n",
       "\n",
       "   pos_VB  pos_WRB  pos_NNP  pos_EX  pos_NNS  pos_SYM  pos_CC  pos_CD  pos_POS  \n",
       "0       0        0        0       0        0        0       0       0        0  \n",
       "1       0        0        0       0        1        0       0       0        0  \n",
       "2       0        0        0       0        0        0       0       0        0  \n",
       "3       0        0        0       0        0        0       0       0        0  \n",
       "4       0        0        0       0        0        0       0       0        0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of X training dataset\n",
    "\n",
    "Create a dictionary where each key is a sentence number and each value is the feature vectors of all the words in that sentence\n",
    "\n",
    "The feature vector of a word will be its word vector, its word features and pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the sentence feature vectors. Each sentence contains a list of all its word feature vectors.\n",
    "df = df.drop(columns=['word'])\n",
    "sentence_feature_vectors = {}\n",
    "for index,row in df.iterrows():\n",
    "    sentence_number = row[0]\n",
    "    word_feature_vector = np.concatenate((row[1:]), axis = None)\n",
    "    if sentence_number in sentence_feature_vectors.keys():\n",
    "        sentence_feature_vectors[sentence_number].append(word_feature_vector)\n",
    "    else:\n",
    "        sentence_feature_vectors[sentence_number] = [word_feature_vector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now, each dictionary key is a sentence number. Each dictionary value is a list of word feature vectors of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Vectors in the first sentence:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 0.05406701, -0.15761112, -0.7017879 ,  0.23409137,  0.49511296,\n",
       "         0.5845138 , -0.21149723,  0.41470313, -0.66746515,  0.3139659 ,\n",
       "        -0.22264327, -0.06002229, -0.26404837,  0.11272451, -0.07870636,\n",
       "        -0.00662771, -0.09953663,  0.19331707, -0.65225816, -0.23743977,\n",
       "         0.22146857,  0.44456705, -0.10762705, -0.02459927,  0.39042968,\n",
       "         0.2107949 ,  0.09701276, -0.1647754 , -0.3796033 ,  0.01939271,\n",
       "        -0.23600912,  0.01130283,  0.2009593 ,  0.05590365,  0.0929635 ,\n",
       "        -0.14136171,  0.04371291, -0.08660819,  0.02132413, -0.14475128,\n",
       "         0.7776699 , -0.3512436 , -0.50120497, -0.04874106,  0.31615117,\n",
       "        -0.07757556,  0.19927543, -0.15894759, -0.02038171,  0.16272163,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       dtype=float32),\n",
       " array([ 0.44699496, -0.6717845 , -1.0470167 ,  0.92995393, -0.10248423,\n",
       "         0.8949343 ,  0.39631328,  0.22533202, -0.66176915, -0.1462245 ,\n",
       "         0.02872628, -0.23922561, -0.22937296,  0.03624375, -0.5052964 ,\n",
       "        -0.2541201 , -0.01356843,  0.11251163, -0.44821602, -0.4158126 ,\n",
       "        -0.01115775, -0.05475885, -0.14416292,  0.03886056, -0.5662192 ,\n",
       "        -0.29925346,  0.15953743, -0.12197897, -0.55408406,  0.29040962,\n",
       "        -0.37411344,  0.0232027 ,  0.19074468,  0.27253956, -0.18640043,\n",
       "         0.00739201,  0.17081603, -0.1073137 ,  0.39374727, -0.19592603,\n",
       "         0.06041001,  0.02363936, -0.41061538,  0.16293938, -0.24179415,\n",
       "         0.03628002,  0.3457169 , -0.00390303, -0.12349886,  0.04952596,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       dtype=float32),\n",
       " array([ 0.23820278,  0.3546044 , -0.92324656,  0.27046227,  0.03173862,\n",
       "         0.5841707 , -0.7109683 ,  0.7086445 , -1.1057305 ,  0.9475153 ,\n",
       "         0.32809123, -0.15646084, -0.84694034, -0.27787343, -0.45676312,\n",
       "         0.3537018 , -0.23278229,  0.30654737, -0.69925165, -0.20054892,\n",
       "         0.1651504 ,  0.3445676 ,  0.4937678 ,  0.28643647,  1.0290054 ,\n",
       "         0.68410903, -0.16124345, -0.39785147,  0.02693672, -0.34354752,\n",
       "         0.03577096, -0.10518081, -0.3204107 , -0.11028051,  0.21729876,\n",
       "         0.31425402,  0.00444317, -0.04805855, -0.2766517 , -0.16521448,\n",
       "         0.5024969 ,  0.719853  , -0.27916294,  0.9951687 , -0.06852672,\n",
       "        -0.48346174,  0.2905371 ,  0.05225617, -0.2080791 , -0.16171472,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       dtype=float32),\n",
       " array([ 0.02532838, -0.05867663,  0.08570069,  0.16157049,  0.27500963,\n",
       "         0.45198894, -0.29694515,  0.42208785, -0.23075293,  0.02101272,\n",
       "        -0.11464824,  0.03001122, -0.09809772,  0.00304593, -0.01951899,\n",
       "        -0.06601331,  0.01443684,  0.02655805,  0.00352346, -0.08548064,\n",
       "         0.00809063,  0.01786366, -0.16654918,  0.07652142,  0.12411135,\n",
       "         0.06388537,  0.1222775 ,  0.1694643 , -0.19152945,  0.1640171 ,\n",
       "        -0.04011254,  0.13121937,  0.15070795,  0.02001256, -0.02354825,\n",
       "         0.09956562,  0.21441793,  0.14910035,  0.23143566, -0.07733449,\n",
       "         0.19787195, -0.06634152, -0.16404991, -0.03839311, -0.3121644 ,\n",
       "        -0.3607757 ,  0.01026546,  0.14020543,  0.23718885, -0.15918069,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       dtype=float32),\n",
       " array([-4.78520155e-01,  5.10107040e-01, -2.87172198e-01,  3.88102293e-01,\n",
       "         1.40472457e-01,  5.69428921e-01, -5.32388687e-01,  4.07803863e-01,\n",
       "        -5.43449521e-01, -7.97000527e-02, -3.88604790e-01, -1.18086398e-01,\n",
       "        -4.63005066e-01,  2.41306946e-01,  1.16238639e-01,  2.50383437e-01,\n",
       "         2.59285569e-01, -2.02821001e-01, -4.59566433e-03, -5.19826226e-02,\n",
       "         3.11385334e-01,  1.65998116e-01, -8.54757652e-02,  1.57675505e-01,\n",
       "         3.00193012e-01, -1.13784686e-01, -5.82933426e-04, -3.40754122e-01,\n",
       "        -1.45239860e-01,  2.11058799e-02, -1.76739410e-01,  2.12009668e-01,\n",
       "         2.40054578e-01,  1.37129053e-02, -1.55773953e-01, -2.90426016e-02,\n",
       "         1.24718279e-01, -1.26364946e-01, -5.45247048e-02,  1.14691779e-01,\n",
       "         9.36251804e-02, -1.62914351e-01, -3.27531278e-01,  1.76050708e-01,\n",
       "        -9.56303850e-02, -1.19807422e-01, -1.33272350e-01, -1.83219798e-02,\n",
       "         8.43155012e-02, -2.09702298e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00], dtype=float32),\n",
       " array([-0.24576187,  0.7825009 , -1.1656063 ,  0.16373083,  0.02832612,\n",
       "         1.2738396 , -0.4270419 ,  0.14545816, -1.1761421 , -0.61445534,\n",
       "         0.2779892 ,  0.3333974 , -0.63845307,  0.58039606, -0.25290775,\n",
       "         0.07898697, -0.2339842 , -0.2577797 , -0.78496903, -0.26258343,\n",
       "         0.18057534,  0.17558897, -0.19377038,  0.21142107,  0.5943702 ,\n",
       "         0.82880235,  0.02111095, -0.30797172,  0.00632412, -0.2714901 ,\n",
       "        -0.41298565,  0.4630808 , -0.03614119,  0.36887038,  0.1200695 ,\n",
       "         0.05307836,  0.3369792 ,  0.02770809, -0.04392501, -0.2977307 ,\n",
       "         0.38170066, -0.06863015, -0.5247401 ,  0.26026547,  0.36673793,\n",
       "        -0.21862699,  0.6795454 ,  0.20904735, -0.20834221,  0.09316103,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       dtype=float32),\n",
       " array([ 0.10888962,  0.23167637,  0.3934712 ,  0.10959324,  0.11657201,\n",
       "         0.31183028, -0.14412907,  0.16273367, -0.36173213,  0.05026492,\n",
       "        -0.05476323, -0.16958939, -0.1692373 , -0.11808292, -0.12403344,\n",
       "        -0.0521758 ,  0.11631706,  0.30285382, -0.2989251 , -0.19366865,\n",
       "         0.07996359,  0.19735545,  0.01783164,  0.31379285,  0.07239787,\n",
       "        -0.14284858,  0.14014979,  0.20215063, -0.08789234,  0.03796547,\n",
       "        -0.04216639,  0.22723894,  0.38481936, -0.20340943, -0.15174633,\n",
       "         0.04204644,  0.02526318,  0.03845844,  0.16676201,  0.07998415,\n",
       "        -0.02852356,  0.04295161, -0.11846794,  0.10135352, -0.26547387,\n",
       "        -0.38473174, -0.07810263, -0.07302706,  0.03554829,  0.09232231,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       dtype=float32),\n",
       " array([ 9.87071320e-02,  1.16424873e-01,  4.80320513e-01,  1.86513111e-01,\n",
       "         4.82229367e-02,  3.01589221e-01, -2.24248424e-01,  3.00035387e-01,\n",
       "        -3.12692642e-01,  3.99046801e-02, -1.42544359e-01, -1.46096110e-01,\n",
       "        -2.27641314e-01, -1.23827532e-01,  1.27900034e-01,  8.85299291e-04,\n",
       "         8.38883817e-02,  4.80097979e-02, -2.44148508e-01, -2.49629498e-01,\n",
       "         3.99503708e-02,  3.52550536e-01, -6.16016239e-02,  2.55473465e-01,\n",
       "         1.57340840e-01, -3.61186638e-02,  2.49173939e-01,  1.92365929e-01,\n",
       "        -3.29201594e-02,  1.34663254e-01, -1.03583805e-01,  5.02391905e-02,\n",
       "         1.09019168e-01,  5.31998202e-02,  4.54185046e-02, -3.06364475e-03,\n",
       "        -2.07836297e-03, -1.29814476e-01,  2.08169430e-01, -5.18538803e-02,\n",
       "         1.51548222e-01,  5.15524521e-02, -2.35466570e-01,  2.63967868e-02,\n",
       "        -7.62109458e-02, -3.85041684e-01, -7.13048577e-02,  9.86813530e-02,\n",
       "        -1.11201800e-01,  1.76625758e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00], dtype=float32),\n",
       " array([ 0.5675778 ,  0.09792379, -0.02665812,  0.37403497, -0.7632306 ,\n",
       "        -0.12687328, -0.07102956, -0.22448592,  0.01379967, -0.17847537,\n",
       "        -0.03584633, -0.05316012,  0.00124046, -0.07511359,  0.1879499 ,\n",
       "         0.01061293, -0.09310209,  0.22149208,  0.27017477, -0.29835832,\n",
       "        -0.14073198,  0.19837052, -0.19875173,  0.20005208, -0.29480726,\n",
       "        -0.095013  , -0.11571279, -0.23327363, -0.210112  ,  0.41564834,\n",
       "        -0.19960208,  0.607368  ,  0.24449544,  0.076914  , -0.1404031 ,\n",
       "        -0.01399096,  0.03838731, -0.37380195,  0.20772833, -0.07716864,\n",
       "        -0.50380963,  0.13159354, -0.25482517,  0.05375935,  0.00488494,\n",
       "         0.3223139 ,  0.2751887 ,  0.2948895 , -0.15474725, -0.23783919,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Feature Vectors in the first sentence:')\n",
    "sentence_feature_vectors[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding and adding each sentence to the input sequence\n",
    "\n",
    "This is done because keras accepts fixed-length input to improve performance by creating tensors of fixed shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad length for sentences and append to the input_sequence \n",
    "\n",
    "#Length of the feature vector \n",
    "dummy_length = len(sentence_feature_vectors[1][0])\n",
    "\n",
    "#Iterate over the sentences, pad them and add them to the input sequence\n",
    "for sentence in sentence_feature_vectors.values():\n",
    "    while len(sentence) < 80:\n",
    "        sentence.append(np.array([0 for zero in range(dummy_length)]))\n",
    "\n",
    "    input_sequence.append(np.array(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.42320919, -0.67043304, -0.28789037, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.2133574 , -0.05606508, -1.07044983, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.09002825,  0.3791883 , -0.61237025, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.05406701, -0.15761112, -0.70178789, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.44699496, -0.67178452, -1.04701674, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.23820278,  0.35460439, -0.92324656, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.42320919, -0.67043304, -0.28789037, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.2133574 , -0.05606508, -1.07044983, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.13553776,  1.2943579 , -1.40821886, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.42320919, -0.67043304, -0.28789037, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.99323946, -0.56385869, -0.4295128 , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.30212063,  0.00744634,  0.46835831, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.05406701, -0.15761112, -0.70178789, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.44699496, -0.67178452, -1.04701674, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.09870713,  0.11642487,  0.48032051, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.68174827, -0.46301931, -0.52813506, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.55528444, -0.75243276, -0.66655385, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.216537  , -0.10798897,  0.28969556, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Input Sequence:')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X training set: (9775, 80, 95)\n",
      "Number of samples (sentences): 9775\n",
      "Number of timesteps (word): 80\n",
      "Number of features (features for each timestep): 95\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X training set: %s' %(x.shape,))\n",
    "print('Number of samples (sentences): {}'.format(x.shape[0]))\n",
    "print('Number of timesteps (word): {}'.format(x.shape[1]))\n",
    "print('Number of features (features for each timestep): {}'.format(x.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Y training dataset\n",
    "\n",
    "Create a dictionary where each key is a sentence number and each value is the targets of that sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the target for each word of the sentence\n",
    "targets = {}\n",
    "for index,row in df_target.iterrows():\n",
    "    sentence_number = row[1]\n",
    "    word_target = row[-1]\n",
    "    if sentence_number in targets.keys():\n",
    "        targets[sentence_number].append(word_target)\n",
    "    else:\n",
    "        targets[sentence_number] = [word_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion of names to index, padding and add each sentence to the output sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the targets to their respective index, pad length for sentences and append to output_sequence\n",
    "for sentence in targets.values():\n",
    "    sentence = [target_to_index[target] for target in sentence]\n",
    "    while len(sentence) < 80:\n",
    "        sentence.append(target_to_index['O'])\n",
    "\n",
    "    output_sequence.append(np.array(sentence))\n",
    "    \n",
    "y = np.array(output_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding of y values\n",
    "\n",
    "CRF needs the input and output sequence to be 3 Dimensional, which is why one-hot encoding is done for the y output values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Sequence:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Output Sequence:')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y training set: (9775, 80, 25)\n",
      "Number of samples (sentences): 9775\n",
      "Number of timesteps (word): 80\n",
      "Number of targets (one-hot): 25\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Y training set: %s' %(y.shape,))\n",
    "print('Number of samples (sentences): {}'.format(y.shape[0]))\n",
    "print('Number of timesteps (word): {}'.format(y.shape[1]))\n",
    "print('Number of targets (one-hot): {}'.format(y.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "Samples refer to the number of sentences we have\n",
    "\n",
    "Timesteps refer to the number of words in each sentence (80 because of padding)\n",
    "\n",
    "Features refer to the features of each word (word vector, capitalise etc.)\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s,y_s = shuffle(x,y,random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_s[:9286]\n",
    "x_test = x_s[9286:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_s[:9286]\n",
    "y_test = y_s[9286:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I allocated a slightly higher proportion of data to training as the amount of data that is available is insufficient for a model to learn very well. Furthermore, the model would be validated on 489 sentences at the end of each epoch based on this proportion, which I feel is sufficient for its validation.\n",
    "\n",
    "I am using the F1 score as the validation metric, thus I need to create my own callback function to perform this validation on the test data at the end of each epoch\n",
    "\n",
    "We also need to get the prediciton on the test set and reshape both y sets so that it would be 2D \n",
    "as sklearn's f1 evaluation only accepts 2D inputs. Just all the words and \n",
    "their corresponding targets, not split into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_shape = y_test.shape\n",
    "y_newshape = (y_shape[0]*y_shape[1], y_shape[-1])\n",
    "y_true_reshaped = np.reshape(y_test, y_newshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compute_f1_Of_Epoch(Callback):\n",
    "    def __init__(self, x_test, y_newshape, y_true_reshaped):\n",
    "        self.x_test = x_test\n",
    "        self.y_newshape = y_newshape\n",
    "        self.y_true_reshaped = y_true_reshaped\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.f1_of_epochs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.x_test) \n",
    "        y_pred_reshaped = np.reshape(y_pred, y_newshape)\n",
    "        self.f1_of_epochs.append(f1_score(y_true_reshaped, y_pred_reshaped, average = 'macro', labels=f1_labels))\n",
    "        print('The f1 score for this epoch is: {}'.format(f1_score(y_true_reshaped, y_pred_reshaped, average = 'macro', labels=f1_labels)))\n",
    "        return\n",
    "\n",
    "compute_f1_of_epoch = Compute_f1_Of_Epoch(x_test=x_test,y_newshape=y_newshape,y_true_reshaped=y_true_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to create base model dynamically\n",
    "#I used a dictionary and formatting to add hidden layers dynamically\n",
    "def base_model(units=50, optimizer='Adam', hidden_layers=2, activation_td ='relu', dropout=0.1, recurrent_dropout=0.1):\n",
    "    hidden_layers_stored = {}\n",
    "    counter=1\n",
    "    input = Input(shape=(x.shape[1],x.shape[-1]))\n",
    "    mask = Masking(mask_value=0.)(input)\n",
    "    for hl in range(hidden_layers):\n",
    "        if counter==1:\n",
    "            hidden_layers_stored['hl_{}'.format(counter)] = Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))(mask)  \n",
    "        else:\n",
    "            hidden_layers_stored['hl_{}'.format(counter)] = Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))(hidden_layers_stored['hl_{}'.format(counter-1)])\n",
    "        counter+=1\n",
    "    model_last_layer = TimeDistributed(Dense(50, activation=activation_td))(hidden_layers_stored['hl_{}'.format(counter-1)])  \n",
    "    crf = CRF(25)  \n",
    "    out = crf(model_last_layer)  \n",
    "    model_final = Model(input, out)\n",
    "    model_final.compile(optimizer=optimizer, loss=crf_loss, metrics=[crf_accuracy])\n",
    "    return model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/250\n",
      "9775/9775 [==============================] - 148s 15ms/step - loss: 28.6072 - crf_accuracy: 0.7036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score for this epoch is: 0.36612599654432953\n",
      "Epoch 2/250\n",
      "9775/9775 [==============================] - 117s 12ms/step - loss: 28.0279 - crf_accuracy: 0.8320\n",
      "The f1 score for this epoch is: 0.5066922766018572\n",
      "Epoch 3/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.9163 - crf_accuracy: 0.8577\n",
      "The f1 score for this epoch is: 0.509393058448147\n",
      "Epoch 4/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.8566 - crf_accuracy: 0.8702\n",
      "The f1 score for this epoch is: 0.5628815507482833\n",
      "Epoch 5/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.8209 - crf_accuracy: 0.8794\n",
      "The f1 score for this epoch is: 0.5798930078267006\n",
      "Epoch 6/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.7943 - crf_accuracy: 0.8839\n",
      "The f1 score for this epoch is: 0.6326960163661294\n",
      "Epoch 7/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.7746 - crf_accuracy: 0.8889\n",
      "The f1 score for this epoch is: 0.6314661115915091\n",
      "Epoch 8/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.7586 - crf_accuracy: 0.8940\n",
      "The f1 score for this epoch is: 0.6717138347088554\n",
      "Epoch 9/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.7461 - crf_accuracy: 0.8963\n",
      "The f1 score for this epoch is: 0.6987576982577345\n",
      "Epoch 10/250\n",
      "9775/9775 [==============================] - 124s 13ms/step - loss: 27.7341 - crf_accuracy: 0.9002\n",
      "The f1 score for this epoch is: 0.7022821584332816\n",
      "Epoch 11/250\n",
      "9775/9775 [==============================] - 128s 13ms/step - loss: 27.7251 - crf_accuracy: 0.9027\n",
      "The f1 score for this epoch is: 0.6974306320361435\n",
      "Epoch 12/250\n",
      "9775/9775 [==============================] - 116s 12ms/step - loss: 27.7152 - crf_accuracy: 0.9061\n",
      "The f1 score for this epoch is: 0.6776355187101716\n",
      "Epoch 13/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.7082 - crf_accuracy: 0.9082\n",
      "The f1 score for this epoch is: 0.7190232758796448\n",
      "Epoch 14/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.6994 - crf_accuracy: 0.9093\n",
      "The f1 score for this epoch is: 0.7205144711540946\n",
      "Epoch 15/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6933 - crf_accuracy: 0.9114\n",
      "The f1 score for this epoch is: 0.7313711189304161\n",
      "Epoch 16/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6895 - crf_accuracy: 0.9128\n",
      "The f1 score for this epoch is: 0.740279539000821\n",
      "Epoch 17/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6827 - crf_accuracy: 0.9136\n",
      "The f1 score for this epoch is: 0.7371015509019859\n",
      "Epoch 18/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6793 - crf_accuracy: 0.9164\n",
      "The f1 score for this epoch is: 0.7360936806433931\n",
      "Epoch 19/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6729 - crf_accuracy: 0.9187\n",
      "The f1 score for this epoch is: 0.7156657521534475\n",
      "Epoch 20/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6701 - crf_accuracy: 0.9195\n",
      "The f1 score for this epoch is: 0.7686949301935879\n",
      "Epoch 21/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6645 - crf_accuracy: 0.9193\n",
      "The f1 score for this epoch is: 0.7482885477977655\n",
      "Epoch 22/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6605 - crf_accuracy: 0.9219\n",
      "The f1 score for this epoch is: 0.7652541349158349\n",
      "Epoch 23/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6563 - crf_accuracy: 0.9236\n",
      "The f1 score for this epoch is: 0.7758080202566727\n",
      "Epoch 24/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6547 - crf_accuracy: 0.9250\n",
      "The f1 score for this epoch is: 0.7974075563361859\n",
      "Epoch 25/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6497 - crf_accuracy: 0.9260\n",
      "The f1 score for this epoch is: 0.7853718660696748\n",
      "Epoch 26/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6484 - crf_accuracy: 0.9260\n",
      "The f1 score for this epoch is: 0.7801013713716306\n",
      "Epoch 27/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6463 - crf_accuracy: 0.9267\n",
      "The f1 score for this epoch is: 0.7663247419629377\n",
      "Epoch 28/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.6417 - crf_accuracy: 0.9282\n",
      "The f1 score for this epoch is: 0.7907595014945453\n",
      "Epoch 29/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6394 - crf_accuracy: 0.9296\n",
      "The f1 score for this epoch is: 0.7951743077628075\n",
      "Epoch 30/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6377 - crf_accuracy: 0.9298\n",
      "The f1 score for this epoch is: 0.8045111078072061\n",
      "Epoch 31/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6346 - crf_accuracy: 0.9303\n",
      "The f1 score for this epoch is: 0.7980876294297218\n",
      "Epoch 32/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.6338 - crf_accuracy: 0.9302\n",
      "The f1 score for this epoch is: 0.8186978937772448\n",
      "Epoch 33/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6302 - crf_accuracy: 0.9310\n",
      "The f1 score for this epoch is: 0.7974538617998882\n",
      "Epoch 34/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.6284 - crf_accuracy: 0.9326\n",
      "The f1 score for this epoch is: 0.7988986252856506\n",
      "Epoch 35/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6274 - crf_accuracy: 0.9333\n",
      "The f1 score for this epoch is: 0.8419573166289033\n",
      "Epoch 36/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6229 - crf_accuracy: 0.9328\n",
      "The f1 score for this epoch is: 0.8262799539564453\n",
      "Epoch 37/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.6220 - crf_accuracy: 0.9343\n",
      "The f1 score for this epoch is: 0.8028500128566142\n",
      "Epoch 38/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6210 - crf_accuracy: 0.9359\n",
      "The f1 score for this epoch is: 0.8242157283448593\n",
      "Epoch 39/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.6190 - crf_accuracy: 0.9349\n",
      "The f1 score for this epoch is: 0.8236217418082862\n",
      "Epoch 40/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6165 - crf_accuracy: 0.9359\n",
      "The f1 score for this epoch is: 0.8280008448025834\n",
      "Epoch 41/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6143 - crf_accuracy: 0.9363\n",
      "The f1 score for this epoch is: 0.8383441790287239\n",
      "Epoch 42/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.6110 - crf_accuracy: 0.9379\n",
      "The f1 score for this epoch is: 0.8267316807464321\n",
      "Epoch 43/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.6119 - crf_accuracy: 0.9367\n",
      "The f1 score for this epoch is: 0.8255380094305987\n",
      "Epoch 44/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.6096 - crf_accuracy: 0.9380\n",
      "The f1 score for this epoch is: 0.8088323528493685\n",
      "Epoch 45/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6079 - crf_accuracy: 0.9392\n",
      "The f1 score for this epoch is: 0.8665198758607543\n",
      "Epoch 46/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6064 - crf_accuracy: 0.9385\n",
      "The f1 score for this epoch is: 0.8578542254081349\n",
      "Epoch 47/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6049 - crf_accuracy: 0.9398\n",
      "The f1 score for this epoch is: 0.8332827020328059\n",
      "Epoch 48/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6027 - crf_accuracy: 0.9410\n",
      "The f1 score for this epoch is: 0.8482241180835883\n",
      "Epoch 49/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6019 - crf_accuracy: 0.9408\n",
      "The f1 score for this epoch is: 0.8443430145956693\n",
      "Epoch 50/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.6002 - crf_accuracy: 0.9416\n",
      "The f1 score for this epoch is: 0.8349894345710149\n",
      "Epoch 51/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5992 - crf_accuracy: 0.9418\n",
      "The f1 score for this epoch is: 0.8402603971889269\n",
      "Epoch 52/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5970 - crf_accuracy: 0.9427\n",
      "The f1 score for this epoch is: 0.851812134375777\n",
      "Epoch 53/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5961 - crf_accuracy: 0.9424\n",
      "The f1 score for this epoch is: 0.8589121749294174\n",
      "Epoch 54/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5949 - crf_accuracy: 0.9426\n",
      "The f1 score for this epoch is: 0.8543624368490969\n",
      "Epoch 55/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5940 - crf_accuracy: 0.9433\n",
      "The f1 score for this epoch is: 0.8592373413365828\n",
      "Epoch 56/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5924 - crf_accuracy: 0.9431\n",
      "The f1 score for this epoch is: 0.8511963256444183\n",
      "Epoch 57/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5916 - crf_accuracy: 0.9440\n",
      "The f1 score for this epoch is: 0.8629525362609746\n",
      "Epoch 58/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5891 - crf_accuracy: 0.9448\n",
      "The f1 score for this epoch is: 0.8729482025340607\n",
      "Epoch 59/250\n",
      "9775/9775 [==============================] - 116s 12ms/step - loss: 27.5878 - crf_accuracy: 0.9463\n",
      "The f1 score for this epoch is: 0.8545016422342556\n",
      "Epoch 60/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5868 - crf_accuracy: 0.9456\n",
      "The f1 score for this epoch is: 0.8771654230840062\n",
      "Epoch 61/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5846 - crf_accuracy: 0.9466\n",
      "The f1 score for this epoch is: 0.8671109285684758\n",
      "Epoch 62/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5863 - crf_accuracy: 0.9457\n",
      "The f1 score for this epoch is: 0.884049249045234\n",
      "Epoch 63/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5851 - crf_accuracy: 0.9463\n",
      "The f1 score for this epoch is: 0.8478463369929691\n",
      "Epoch 64/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5828 - crf_accuracy: 0.9475\n",
      "The f1 score for this epoch is: 0.8815855009728025\n",
      "Epoch 65/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5831 - crf_accuracy: 0.9474\n",
      "The f1 score for this epoch is: 0.8808072763454048\n",
      "Epoch 66/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5812 - crf_accuracy: 0.9481\n",
      "The f1 score for this epoch is: 0.8749272248240428\n",
      "Epoch 67/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5794 - crf_accuracy: 0.9476\n",
      "The f1 score for this epoch is: 0.8936607489939568\n",
      "Epoch 68/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5809 - crf_accuracy: 0.9478\n",
      "The f1 score for this epoch is: 0.8626084121953221\n",
      "Epoch 69/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5781 - crf_accuracy: 0.9475\n",
      "The f1 score for this epoch is: 0.8472818069360316\n",
      "Epoch 70/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5759 - crf_accuracy: 0.9501\n",
      "The f1 score for this epoch is: 0.8821191151539723\n",
      "Epoch 71/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5756 - crf_accuracy: 0.9491\n",
      "The f1 score for this epoch is: 0.8767222351738267\n",
      "Epoch 72/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5752 - crf_accuracy: 0.9498\n",
      "The f1 score for this epoch is: 0.8933445439275891\n",
      "Epoch 73/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5745 - crf_accuracy: 0.9496\n",
      "The f1 score for this epoch is: 0.8850898741611114\n",
      "Epoch 74/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5730 - crf_accuracy: 0.9505\n",
      "The f1 score for this epoch is: 0.8690747623793932\n",
      "Epoch 75/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5714 - crf_accuracy: 0.9507\n",
      "The f1 score for this epoch is: 0.8647738495363022\n",
      "Epoch 76/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5716 - crf_accuracy: 0.9507\n",
      "The f1 score for this epoch is: 0.8742613501859257\n",
      "Epoch 77/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5708 - crf_accuracy: 0.9514\n",
      "The f1 score for this epoch is: 0.8914333093105431\n",
      "Epoch 78/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5695 - crf_accuracy: 0.9534\n",
      "The f1 score for this epoch is: 0.882362198822276\n",
      "Epoch 79/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5695 - crf_accuracy: 0.9522\n",
      "The f1 score for this epoch is: 0.8836682056185508\n",
      "Epoch 80/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5687 - crf_accuracy: 0.9523\n",
      "The f1 score for this epoch is: 0.8800898790940131\n",
      "Epoch 81/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5658 - crf_accuracy: 0.9542\n",
      "The f1 score for this epoch is: 0.890302737134694\n",
      "Epoch 82/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5659 - crf_accuracy: 0.9540\n",
      "The f1 score for this epoch is: 0.8896878441793618\n",
      "Epoch 83/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5634 - crf_accuracy: 0.9540\n",
      "The f1 score for this epoch is: 0.8894761792297153\n",
      "Epoch 84/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5647 - crf_accuracy: 0.9538\n",
      "The f1 score for this epoch is: 0.8970716589209583\n",
      "Epoch 85/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5648 - crf_accuracy: 0.9535\n",
      "The f1 score for this epoch is: 0.9016771476773539\n",
      "Epoch 86/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5627 - crf_accuracy: 0.9549\n",
      "The f1 score for this epoch is: 0.8974910089339726\n",
      "Epoch 87/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5616 - crf_accuracy: 0.9553\n",
      "The f1 score for this epoch is: 0.8776550675959722\n",
      "Epoch 88/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5618 - crf_accuracy: 0.9557\n",
      "The f1 score for this epoch is: 0.891474655211168\n",
      "Epoch 89/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5600 - crf_accuracy: 0.9546\n",
      "The f1 score for this epoch is: 0.9049017990885445\n",
      "Epoch 90/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5586 - crf_accuracy: 0.9550\n",
      "The f1 score for this epoch is: 0.90083591664528\n",
      "Epoch 91/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5592 - crf_accuracy: 0.9554\n",
      "The f1 score for this epoch is: 0.9101315803994164\n",
      "Epoch 92/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5597 - crf_accuracy: 0.9559\n",
      "The f1 score for this epoch is: 0.8974023047837848\n",
      "Epoch 93/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5567 - crf_accuracy: 0.9566\n",
      "The f1 score for this epoch is: 0.9060595408182159\n",
      "Epoch 94/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5589 - crf_accuracy: 0.9566\n",
      "The f1 score for this epoch is: 0.9006960544444697\n",
      "Epoch 95/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5570 - crf_accuracy: 0.9562\n",
      "The f1 score for this epoch is: 0.8870122512709937\n",
      "Epoch 96/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5565 - crf_accuracy: 0.9565\n",
      "The f1 score for this epoch is: 0.9018619079967998\n",
      "Epoch 97/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5572 - crf_accuracy: 0.9565\n",
      "The f1 score for this epoch is: 0.9068299550686145\n",
      "Epoch 98/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5548 - crf_accuracy: 0.9582\n",
      "The f1 score for this epoch is: 0.9001558055358249\n",
      "Epoch 99/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5530 - crf_accuracy: 0.9579\n",
      "The f1 score for this epoch is: 0.8899467366161818\n",
      "Epoch 100/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5539 - crf_accuracy: 0.9591\n",
      "The f1 score for this epoch is: 0.9049036122012328\n",
      "Epoch 101/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5534 - crf_accuracy: 0.9573\n",
      "The f1 score for this epoch is: 0.9042274147643906\n",
      "Epoch 102/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5539 - crf_accuracy: 0.9588\n",
      "The f1 score for this epoch is: 0.8967597098332775\n",
      "Epoch 103/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5513 - crf_accuracy: 0.9588\n",
      "The f1 score for this epoch is: 0.9043958413834347\n",
      "Epoch 104/250\n",
      "9775/9775 [==============================] - 120s 12ms/step - loss: 27.5528 - crf_accuracy: 0.9581\n",
      "The f1 score for this epoch is: 0.9059920558853104\n",
      "Epoch 105/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5497 - crf_accuracy: 0.9596\n",
      "The f1 score for this epoch is: 0.9094384030016065\n",
      "Epoch 106/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5487 - crf_accuracy: 0.9596\n",
      "The f1 score for this epoch is: 0.9068363445902943\n",
      "Epoch 107/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5480 - crf_accuracy: 0.9603\n",
      "The f1 score for this epoch is: 0.9027166785136725\n",
      "Epoch 108/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5499 - crf_accuracy: 0.9598\n",
      "The f1 score for this epoch is: 0.9056595439702179\n",
      "Epoch 109/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5495 - crf_accuracy: 0.9597\n",
      "The f1 score for this epoch is: 0.9090054286731845\n",
      "Epoch 110/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5473 - crf_accuracy: 0.9613\n",
      "The f1 score for this epoch is: 0.9137310716645773\n",
      "Epoch 111/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5479 - crf_accuracy: 0.9592\n",
      "The f1 score for this epoch is: 0.9069003662945004\n",
      "Epoch 112/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5451 - crf_accuracy: 0.9620\n",
      "The f1 score for this epoch is: 0.9133690034467716\n",
      "Epoch 113/250\n",
      "9775/9775 [==============================] - 116s 12ms/step - loss: 27.5473 - crf_accuracy: 0.9605\n",
      "The f1 score for this epoch is: 0.9132317816596719\n",
      "Epoch 114/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5458 - crf_accuracy: 0.9612\n",
      "The f1 score for this epoch is: 0.9104995464337505\n",
      "Epoch 115/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5452 - crf_accuracy: 0.9618\n",
      "The f1 score for this epoch is: 0.9159413009634418\n",
      "Epoch 116/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5442 - crf_accuracy: 0.9617\n",
      "The f1 score for this epoch is: 0.9099241528310644\n",
      "Epoch 117/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5437 - crf_accuracy: 0.9630\n",
      "The f1 score for this epoch is: 0.9101850889387073\n",
      "Epoch 118/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5438 - crf_accuracy: 0.9619\n",
      "The f1 score for this epoch is: 0.9043402889569757\n",
      "Epoch 119/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5435 - crf_accuracy: 0.9620\n",
      "The f1 score for this epoch is: 0.921250177702861\n",
      "Epoch 120/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5421 - crf_accuracy: 0.9631\n",
      "The f1 score for this epoch is: 0.9071737221947475\n",
      "Epoch 121/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5419 - crf_accuracy: 0.9641\n",
      "The f1 score for this epoch is: 0.9160035085609858\n",
      "Epoch 122/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5414 - crf_accuracy: 0.9631\n",
      "The f1 score for this epoch is: 0.9148245015831433\n",
      "Epoch 123/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5394 - crf_accuracy: 0.9642\n",
      "The f1 score for this epoch is: 0.9168688929488932\n",
      "Epoch 124/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5402 - crf_accuracy: 0.9635\n",
      "The f1 score for this epoch is: 0.9154479120576774\n",
      "Epoch 125/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5396 - crf_accuracy: 0.9635\n",
      "The f1 score for this epoch is: 0.9179097553486107\n",
      "Epoch 126/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5390 - crf_accuracy: 0.9646\n",
      "The f1 score for this epoch is: 0.9057364737591888\n",
      "Epoch 127/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5393 - crf_accuracy: 0.9645\n",
      "The f1 score for this epoch is: 0.9136484340735368\n",
      "Epoch 128/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5387 - crf_accuracy: 0.9654\n",
      "The f1 score for this epoch is: 0.9195490123887154\n",
      "Epoch 129/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5372 - crf_accuracy: 0.9658\n",
      "The f1 score for this epoch is: 0.9155382822009401\n",
      "Epoch 130/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5376 - crf_accuracy: 0.9658\n",
      "The f1 score for this epoch is: 0.9054799306319964\n",
      "Epoch 131/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5377 - crf_accuracy: 0.9645\n",
      "The f1 score for this epoch is: 0.9153636112180233\n",
      "Epoch 132/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5361 - crf_accuracy: 0.9650\n",
      "The f1 score for this epoch is: 0.9129099470379153\n",
      "Epoch 133/250\n",
      "9775/9775 [==============================] - 126s 13ms/step - loss: 27.5376 - crf_accuracy: 0.9656\n",
      "The f1 score for this epoch is: 0.9032574441659612\n",
      "Epoch 134/250\n",
      "9775/9775 [==============================] - 130s 13ms/step - loss: 27.5359 - crf_accuracy: 0.9659\n",
      "The f1 score for this epoch is: 0.9191541010083975\n",
      "Epoch 135/250\n",
      "9775/9775 [==============================] - 121s 12ms/step - loss: 27.5373 - crf_accuracy: 0.9655\n",
      "The f1 score for this epoch is: 0.910390311544203\n",
      "Epoch 136/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5355 - crf_accuracy: 0.9655\n",
      "The f1 score for this epoch is: 0.9231083716670838\n",
      "Epoch 137/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5340 - crf_accuracy: 0.9661\n",
      "The f1 score for this epoch is: 0.916966498363896\n",
      "Epoch 138/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5340 - crf_accuracy: 0.9664\n",
      "The f1 score for this epoch is: 0.9174434188222618\n",
      "Epoch 139/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5366 - crf_accuracy: 0.9659\n",
      "The f1 score for this epoch is: 0.919251621769578\n",
      "Epoch 140/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5345 - crf_accuracy: 0.9668\n",
      "The f1 score for this epoch is: 0.9186316642654759\n",
      "Epoch 141/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5346 - crf_accuracy: 0.9661\n",
      "The f1 score for this epoch is: 0.9082176261440166\n",
      "Epoch 142/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5330 - crf_accuracy: 0.9667\n",
      "The f1 score for this epoch is: 0.9228329592897074\n",
      "Epoch 143/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5342 - crf_accuracy: 0.9663\n",
      "The f1 score for this epoch is: 0.9215038074830928\n",
      "Epoch 144/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5333 - crf_accuracy: 0.9661\n",
      "The f1 score for this epoch is: 0.9076489381033772\n",
      "Epoch 145/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5327 - crf_accuracy: 0.9667\n",
      "The f1 score for this epoch is: 0.9120497630423198\n",
      "Epoch 146/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5324 - crf_accuracy: 0.9664\n",
      "The f1 score for this epoch is: 0.916204506262884\n",
      "Epoch 147/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5326 - crf_accuracy: 0.9681\n",
      "The f1 score for this epoch is: 0.9116014812129243\n",
      "Epoch 148/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5306 - crf_accuracy: 0.9685\n",
      "The f1 score for this epoch is: 0.91741931535495\n",
      "Epoch 149/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5322 - crf_accuracy: 0.9683\n",
      "The f1 score for this epoch is: 0.9212808976573287\n",
      "Epoch 150/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5302 - crf_accuracy: 0.9680\n",
      "The f1 score for this epoch is: 0.9255496412495742\n",
      "Epoch 151/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5299 - crf_accuracy: 0.9686\n",
      "The f1 score for this epoch is: 0.9176819145591866\n",
      "Epoch 152/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5308 - crf_accuracy: 0.9688\n",
      "The f1 score for this epoch is: 0.9148169830341105\n",
      "Epoch 153/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5288 - crf_accuracy: 0.9700\n",
      "The f1 score for this epoch is: 0.9226334516845922\n",
      "Epoch 154/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5291 - crf_accuracy: 0.9690\n",
      "The f1 score for this epoch is: 0.9125892728311773\n",
      "Epoch 155/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5304 - crf_accuracy: 0.9681\n",
      "The f1 score for this epoch is: 0.9226028794230134\n",
      "Epoch 156/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5296 - crf_accuracy: 0.9690\n",
      "The f1 score for this epoch is: 0.9154918131875763\n",
      "Epoch 157/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5304 - crf_accuracy: 0.9671\n",
      "The f1 score for this epoch is: 0.9156835242895145\n",
      "Epoch 158/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5283 - crf_accuracy: 0.9687\n",
      "The f1 score for this epoch is: 0.9138082567199716\n",
      "Epoch 159/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5268 - crf_accuracy: 0.9711\n",
      "The f1 score for this epoch is: 0.9164233041883745\n",
      "Epoch 160/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5282 - crf_accuracy: 0.9686\n",
      "The f1 score for this epoch is: 0.926112565515953\n",
      "Epoch 161/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5277 - crf_accuracy: 0.9697\n",
      "The f1 score for this epoch is: 0.9342177662467007\n",
      "Epoch 162/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5268 - crf_accuracy: 0.9707\n",
      "The f1 score for this epoch is: 0.9278359316335759\n",
      "Epoch 163/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5262 - crf_accuracy: 0.9705\n",
      "The f1 score for this epoch is: 0.9248058021030152\n",
      "Epoch 164/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5266 - crf_accuracy: 0.9703\n",
      "The f1 score for this epoch is: 0.9218996106901202\n",
      "Epoch 165/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5271 - crf_accuracy: 0.9703\n",
      "The f1 score for this epoch is: 0.918227039276247\n",
      "Epoch 166/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5247 - crf_accuracy: 0.9714\n",
      "The f1 score for this epoch is: 0.9306284801627195\n",
      "Epoch 167/250\n",
      "9775/9775 [==============================] - 122s 13ms/step - loss: 27.5257 - crf_accuracy: 0.9708\n",
      "The f1 score for this epoch is: 0.9200046793576916\n",
      "Epoch 168/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5241 - crf_accuracy: 0.9711\n",
      "The f1 score for this epoch is: 0.9111134366994915\n",
      "Epoch 169/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5257 - crf_accuracy: 0.9699\n",
      "The f1 score for this epoch is: 0.9146406597290174\n",
      "Epoch 170/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5246 - crf_accuracy: 0.9705\n",
      "The f1 score for this epoch is: 0.9129655490256979\n",
      "Epoch 171/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5248 - crf_accuracy: 0.9701\n",
      "The f1 score for this epoch is: 0.9169856077330083\n",
      "Epoch 172/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5244 - crf_accuracy: 0.9716\n",
      "The f1 score for this epoch is: 0.9206071179954068\n",
      "Epoch 173/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5255 - crf_accuracy: 0.9703\n",
      "The f1 score for this epoch is: 0.9324672138291668\n",
      "Epoch 174/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5248 - crf_accuracy: 0.9709\n",
      "The f1 score for this epoch is: 0.9250617837500759\n",
      "Epoch 175/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5232 - crf_accuracy: 0.9718\n",
      "The f1 score for this epoch is: 0.918525891666231\n",
      "Epoch 176/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5237 - crf_accuracy: 0.9717\n",
      "The f1 score for this epoch is: 0.918137321082867\n",
      "Epoch 177/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5217 - crf_accuracy: 0.9720\n",
      "The f1 score for this epoch is: 0.9311714449792738\n",
      "Epoch 178/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5238 - crf_accuracy: 0.9724\n",
      "The f1 score for this epoch is: 0.9368641953618919\n",
      "Epoch 179/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5231 - crf_accuracy: 0.9724\n",
      "The f1 score for this epoch is: 0.9235767993193105\n",
      "Epoch 180/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5242 - crf_accuracy: 0.9704\n",
      "The f1 score for this epoch is: 0.9169687660828578\n",
      "Epoch 181/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5217 - crf_accuracy: 0.9728\n",
      "The f1 score for this epoch is: 0.9346736002140416\n",
      "Epoch 182/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5221 - crf_accuracy: 0.9730\n",
      "The f1 score for this epoch is: 0.9272740669781587\n",
      "Epoch 183/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5223 - crf_accuracy: 0.9725\n",
      "The f1 score for this epoch is: 0.9207794630543896\n",
      "Epoch 184/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5227 - crf_accuracy: 0.9722\n",
      "The f1 score for this epoch is: 0.9350228406854653\n",
      "Epoch 185/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5205 - crf_accuracy: 0.9738\n",
      "The f1 score for this epoch is: 0.9305418460342577\n",
      "Epoch 186/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5217 - crf_accuracy: 0.9722\n",
      "The f1 score for this epoch is: 0.9215635366548757\n",
      "Epoch 187/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5208 - crf_accuracy: 0.9734\n",
      "The f1 score for this epoch is: 0.9341147573025917\n",
      "Epoch 188/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5211 - crf_accuracy: 0.9732\n",
      "The f1 score for this epoch is: 0.9382543282337187\n",
      "Epoch 189/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5207 - crf_accuracy: 0.9729\n",
      "The f1 score for this epoch is: 0.9225691424288159\n",
      "Epoch 190/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5192 - crf_accuracy: 0.9738\n",
      "The f1 score for this epoch is: 0.9268269116026944\n",
      "Epoch 191/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5209 - crf_accuracy: 0.9733\n",
      "The f1 score for this epoch is: 0.9281604114373018\n",
      "Epoch 192/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5207 - crf_accuracy: 0.9735\n",
      "The f1 score for this epoch is: 0.9329808051076403\n",
      "Epoch 193/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5189 - crf_accuracy: 0.9747\n",
      "The f1 score for this epoch is: 0.9275803594543914\n",
      "Epoch 194/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5197 - crf_accuracy: 0.9735\n",
      "The f1 score for this epoch is: 0.9388074923540364\n",
      "Epoch 195/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5193 - crf_accuracy: 0.9744\n",
      "The f1 score for this epoch is: 0.924448736980129\n",
      "Epoch 196/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5186 - crf_accuracy: 0.9750\n",
      "The f1 score for this epoch is: 0.933686730788916\n",
      "Epoch 197/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5183 - crf_accuracy: 0.9751\n",
      "The f1 score for this epoch is: 0.9359192560920734\n",
      "Epoch 198/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5173 - crf_accuracy: 0.9745\n",
      "The f1 score for this epoch is: 0.9363807475232898\n",
      "Epoch 199/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5175 - crf_accuracy: 0.9747\n",
      "The f1 score for this epoch is: 0.93569725793806\n",
      "Epoch 200/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5183 - crf_accuracy: 0.9736\n",
      "The f1 score for this epoch is: 0.9330976034166393\n",
      "Epoch 201/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5176 - crf_accuracy: 0.9739\n",
      "The f1 score for this epoch is: 0.9178164077504949\n",
      "Epoch 202/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5168 - crf_accuracy: 0.9743\n",
      "The f1 score for this epoch is: 0.9219180016894132\n",
      "Epoch 203/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5159 - crf_accuracy: 0.9749\n",
      "The f1 score for this epoch is: 0.9209487123091269\n",
      "Epoch 204/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5172 - crf_accuracy: 0.9747\n",
      "The f1 score for this epoch is: 0.9324490043673457\n",
      "Epoch 205/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5169 - crf_accuracy: 0.9754\n",
      "The f1 score for this epoch is: 0.9219377942229624\n",
      "Epoch 206/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5177 - crf_accuracy: 0.9745\n",
      "The f1 score for this epoch is: 0.9353971484048271\n",
      "Epoch 207/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5172 - crf_accuracy: 0.9744\n",
      "The f1 score for this epoch is: 0.9238722040431449\n",
      "Epoch 208/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5171 - crf_accuracy: 0.9752\n",
      "The f1 score for this epoch is: 0.9322506304213675\n",
      "Epoch 209/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5159 - crf_accuracy: 0.9761\n",
      "The f1 score for this epoch is: 0.9224619828952513\n",
      "Epoch 210/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5149 - crf_accuracy: 0.9761\n",
      "The f1 score for this epoch is: 0.9238963741505586\n",
      "Epoch 211/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5150 - crf_accuracy: 0.9750\n",
      "The f1 score for this epoch is: 0.9275623443634284\n",
      "Epoch 212/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5150 - crf_accuracy: 0.9758\n",
      "The f1 score for this epoch is: 0.9244954924917211\n",
      "Epoch 213/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5157 - crf_accuracy: 0.9750\n",
      "The f1 score for this epoch is: 0.9385553546973675\n",
      "Epoch 214/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5158 - crf_accuracy: 0.9757\n",
      "The f1 score for this epoch is: 0.9330266565737985\n",
      "Epoch 215/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5153 - crf_accuracy: 0.9756\n",
      "The f1 score for this epoch is: 0.9386868049466027\n",
      "Epoch 216/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5151 - crf_accuracy: 0.9759\n",
      "The f1 score for this epoch is: 0.9239691075063808\n",
      "Epoch 217/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5153 - crf_accuracy: 0.9761\n",
      "The f1 score for this epoch is: 0.9212822751252331\n",
      "Epoch 218/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5163 - crf_accuracy: 0.9758\n",
      "The f1 score for this epoch is: 0.931559495807002\n",
      "Epoch 219/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5152 - crf_accuracy: 0.9768\n",
      "The f1 score for this epoch is: 0.9241500718904017\n",
      "Epoch 220/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5150 - crf_accuracy: 0.9753\n",
      "The f1 score for this epoch is: 0.9243467716654861\n",
      "Epoch 221/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5147 - crf_accuracy: 0.9766\n",
      "The f1 score for this epoch is: 0.9388236573558576\n",
      "Epoch 222/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5153 - crf_accuracy: 0.9760\n",
      "The f1 score for this epoch is: 0.9251953583911069\n",
      "Epoch 223/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5138 - crf_accuracy: 0.9764\n",
      "The f1 score for this epoch is: 0.9267016046629163\n",
      "Epoch 224/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5141 - crf_accuracy: 0.9756\n",
      "The f1 score for this epoch is: 0.9249389292151925\n",
      "Epoch 225/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5133 - crf_accuracy: 0.9769\n",
      "The f1 score for this epoch is: 0.9290155955435901\n",
      "Epoch 226/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5138 - crf_accuracy: 0.9773\n",
      "The f1 score for this epoch is: 0.936594006001307\n",
      "Epoch 227/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5127 - crf_accuracy: 0.9771\n",
      "The f1 score for this epoch is: 0.9372415563931279\n",
      "Epoch 228/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5113 - crf_accuracy: 0.9776\n",
      "The f1 score for this epoch is: 0.924896508826544\n",
      "Epoch 229/250\n",
      "9775/9775 [==============================] - 115s 12ms/step - loss: 27.5133 - crf_accuracy: 0.9763\n",
      "The f1 score for this epoch is: 0.9260962874387139\n",
      "Epoch 230/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5118 - crf_accuracy: 0.9774\n",
      "The f1 score for this epoch is: 0.9273035137250983\n",
      "Epoch 231/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5117 - crf_accuracy: 0.9774\n",
      "The f1 score for this epoch is: 0.9305622417031092\n",
      "Epoch 232/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5115 - crf_accuracy: 0.9780\n",
      "The f1 score for this epoch is: 0.9426733168208864\n",
      "Epoch 233/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5115 - crf_accuracy: 0.9787\n",
      "The f1 score for this epoch is: 0.9442398099593845\n",
      "Epoch 234/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5122 - crf_accuracy: 0.9779\n",
      "The f1 score for this epoch is: 0.9309688364659161\n",
      "Epoch 235/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5130 - crf_accuracy: 0.9769\n",
      "The f1 score for this epoch is: 0.9410271869187975\n",
      "Epoch 236/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5107 - crf_accuracy: 0.9783\n",
      "The f1 score for this epoch is: 0.9271545244446333\n",
      "Epoch 237/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5116 - crf_accuracy: 0.9787\n",
      "The f1 score for this epoch is: 0.9369907029105781\n",
      "Epoch 238/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5109 - crf_accuracy: 0.9779\n",
      "The f1 score for this epoch is: 0.9427348516989505\n",
      "Epoch 239/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5101 - crf_accuracy: 0.9785\n",
      "The f1 score for this epoch is: 0.9386553423799974\n",
      "Epoch 240/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5106 - crf_accuracy: 0.9788\n",
      "The f1 score for this epoch is: 0.9313673567795404\n",
      "Epoch 241/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5119 - crf_accuracy: 0.9781\n",
      "The f1 score for this epoch is: 0.9306681990628739\n",
      "Epoch 242/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5117 - crf_accuracy: 0.9778\n",
      "The f1 score for this epoch is: 0.944322649590545\n",
      "Epoch 243/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5107 - crf_accuracy: 0.9789\n",
      "The f1 score for this epoch is: 0.9394440321056715\n",
      "Epoch 244/250\n",
      "9775/9775 [==============================] - 116s 12ms/step - loss: 27.5115 - crf_accuracy: 0.9779\n",
      "The f1 score for this epoch is: 0.9361119028558033\n",
      "Epoch 245/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5089 - crf_accuracy: 0.9789\n",
      "The f1 score for this epoch is: 0.9414198044989294\n",
      "Epoch 246/250\n",
      "9775/9775 [==============================] - 113s 12ms/step - loss: 27.5108 - crf_accuracy: 0.9785\n",
      "The f1 score for this epoch is: 0.9271417886764916\n",
      "Epoch 247/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5084 - crf_accuracy: 0.9798\n",
      "The f1 score for this epoch is: 0.9452191428796907\n",
      "Epoch 248/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5108 - crf_accuracy: 0.9789\n",
      "The f1 score for this epoch is: 0.9290995035944235\n",
      "Epoch 249/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5106 - crf_accuracy: 0.9779\n",
      "The f1 score for this epoch is: 0.9381941569052968\n",
      "Epoch 250/250\n",
      "9775/9775 [==============================] - 114s 12ms/step - loss: 27.5101 - crf_accuracy: 0.9786\n",
      "The f1 score for this epoch is: 0.9456862101883264\n"
     ]
    }
   ],
   "source": [
    "#GPU Options are added to prevent the program from taking up all the computer GPU's memory. \n",
    "graph_trainer = tf.Graph()\n",
    "with graph_trainer.as_default():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session_trainer = tf.Session(config=config)\n",
    "    with session_trainer.as_default():\n",
    "        model_trainer = base_model(units=best_hyperparameter_info[1]['units_hyperparams'],optimizer=best_hyperparameter_info[1]['optimizer_hyperparams'],hidden_layers=best_hyperparameter_info[1]['hidden_layers_hyperparams'],dropout=best_hyperparameter_info[1]['dropout_hyperparams'],recurrent_dropout=best_hyperparameter_info[1]['recurrent_dropout_hyperparams'])\n",
    "        model_trainer._make_predict_function()\n",
    "        model_trainer.fit(x_s, y_s, epochs=best_hyperparameter_info[1]['epochs_hyperparams'], batch_size=best_hyperparameter_info[1]['batch_size_hyperparams'], callbacks=[save_weights, compute_f1_of_epoch])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the validation f1 scores at the end of each epoch, we can see which model's hyperparameters are most optimal \n",
    "for our use case by picking out the one with the highest validation f1.\n",
    "\n",
    "We then iterate through the folder of all saved model hyperparameters and only save the one that is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_hist = list(compute_f1_of_epoch.f1_of_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The names of the first weight saved to the model training weights folder are not zero index based, it starts from 1\n",
    "#example: weights.01.hdf5, weights.02.hdf5\n",
    "best_epoch = f1_hist.index(max(f1_hist))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_score = max(f1_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9456862101883264"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_epoch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for i in range(1,251):\n",
    "    if i==best_epoch:\n",
    "        continue\n",
    "    else:\n",
    "        epoch_number = str(i)\n",
    "        if len(epoch_number) < 2:\n",
    "            #pads 0s in front to match model weight numbers less than 10\n",
    "            epoch_number = epoch_number.zfill(2) \n",
    "\n",
    "        os.remove(os.path.join(curr_dir,'model_training_weights','weights.{}.hdf5').format(epoch_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f1_hist_filepath, \"wb\") as t:\n",
    "    pickle.dump(f1_hist, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the best weight to weights.best.hdf5\n",
    "os.rename(os.path.join(curr_dir,'model_training_weights','weights.{}.hdf5').format(best_epoch),os.path.join(curr_dir,'model_training_weights','weights.{}.hdf5').format(\"best\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Directional LSTM (Long Short Term Memory) with CRF (Conditional Random Fields)\n",
    "\n",
    "**Why?**\n",
    "Suitable for learning sequence-related data, sequence is very important in sentences. For example:\n",
    "<br>\n",
    "Give example\n",
    "<br>\n",
    "\n",
    "We can guess an entity from the context given\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Recurrent Neural Networks\n",
    "Feeds the information of previous time steps into the current time step to help with the prediction of the current time step\n",
    "\n",
    "**Problem:** Cannot learn long term dependencies i.e. words very early in the sentence cannot be used to help with the prediction of a word that occurs much later in the sentence. This is because of the vanshing gradient problem which arises because of matrice multiplications in RNN back-propagation through time.\n",
    "\n",
    "### Vanilla LSTM Concept\n",
    "\n",
    "![](https://i.stack.imgur.com/swN2l.png)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "* Hidden layer is made of hidden units \n",
    "  * Learning is done here\n",
    "  * Each hidden unit learns different things about the sequence\n",
    "  * Each hidden unit can be thought of as a chain of LSTM cells due to its recurrent nature\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "What an LSTM unit looks like:\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Parts of an LSTM cell:\n",
    "* Cell State (Black line at the top)\n",
    "  * LSTM's memory running throughout its recursive operations\n",
    "* Hidden State\n",
    "  * Filtered version of the cell state, contain information related to the prediction of the currenr time step that is useful for the next time step\n",
    "* Decision Gates (Yellow boxes)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### LSTM Decision Gates\n",
    "\n",
    "There are 3 decision gates in a LSTM cell: *Forget, Input, Output*\n",
    "\n",
    "#### Forget (Remember) Gate\n",
    "* **Decides** what past information is irrelevant for learning the sequence for future timesteps\n",
    "* **Decides** what information is still important for learning the sequence for future timesteps\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)\n",
    "\n",
    "1. Takes in prev hidden state and current input as its inputs\n",
    "2. LSTM weights are multiplied with their respective inputs\n",
    "3. Bias is added to each of the results\n",
    "5. Results are summed and put through a sigmoid function\n",
    "\n",
    "\n",
    "The sigmoid function outputs the values between 0 and 1. Values closer to 0 are deemed more negligible vice-versa.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Input (Update) Gate\n",
    "* **Decides** what information to add to the cell state that can be useful for the LSTM’s learnings for the future sequence \n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)\n",
    "\n",
    "First equation decides what to update and how much to update the candidate values. The second equation generates all potential candidate values to be added. The output of these two equations are point-wise multiplied to output the scaled values to be updated.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Cell State Operation (Forget Gate & Input Gate)\n",
    "* **Execution** of decisions made by *forget* and *input* gate\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)\n",
    "\n",
    "<br>\n",
    "Execution of decision made by forget gate:\n",
    "\n",
    "The forget gate’s output is pointwise multiplied by the cell state, this is where we execute the decision made by the forget gate and ‘forget’ insignificant information from the cell state. Cell state values multiplied by values closer to 0 are more likely to be ‘forgotten’ vice-versa\n",
    "\n",
    "<br>\n",
    "Execution of decision made by input gate:\n",
    "\n",
    "The cell state is then pointwise added with the input gate’s output to execute the update decision made by the input gate\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Output Gate\n",
    "* **Decides** what information we are going to output from the cell state that can be useful for the next time step\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Cell State Operation (Output Gate):\n",
    "* **Execution** of decision made by *output* gate\n",
    "\n",
    "The first equation decides what values to output from the cell state. The second equation pointwise multiplies the first equation (decision) by a tahn function applied to the cell state which results in the hidden state.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Bidirectional\n",
    "\n",
    "Same operations but runs the sequence backwards. \n",
    "\n",
    "Able to preserve information from past and future = context\n",
    "\n",
    "_\"You shall know a word by the company it keeps\" - John Rupert Firth_\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### CRF\n",
    "\n",
    "![](https://d2ueix13hy5h3i.cloudfront.net/wp-content/uploads/2019/06/CodeCogsEqn7.png)\n",
    "\n",
    "Calculating Probability of the most likely Y sequence of labels given the X sequence\n",
    "* f represents the feature functions\n",
    "* inner sum sums the feature functions of the words in the sentence\n",
    "* outer sum sums the feature functions of the sentences\n",
    "* exp is an expotential function and 1/Z(x) is a normalization which helps make it a probability\n",
    "\n",
    "![](https://d2ueix13hy5h3i.cloudfront.net/wp-content/uploads/2019/06/CodeCogsEqn1-5.png)\n",
    "![](https://d2ueix13hy5h3i.cloudfront.net/wp-content/uploads/2019/06/ss1-2.png)\n",
    "\n",
    "The feature function takes in the previous label and the current label as well as the current input into consideration when making a prediction\n",
    "\n",
    "<br>\n",
    "\n",
    "### Back propagation through time (Training)\n",
    "\n",
    "Process where the LSTM neural network updates all its parameters (weights and biases) to minimize the loss function. The loss function is the magnitude of errors that the LSTM neural network makes during prediction. Backpropagation through time can be thought of as the process of constantly finding the right direction in which to adjust the weights such that the local minimum of the loss function can be reached, which will make the LSTM neural network predict better.\n",
    "<br>\n",
    "The loss function we use is the CRF loss function which is a negative log-likelihood function.\n",
    "* Likelihood: Measures how well the parameters have adjusted to classify our entity correctly using the probabilities produced by the model\n",
    "* Log function: Regulate values as some of the likelihood values can be very small\n",
    "* Negative: Minimise the function negative so that we can maximise our classification performance by minimizing the function as the optimizers are made to minimize loss functions\n",
    "\n",
    "The closer our loss function to 0, the better\n",
    "\n",
    "\n",
    "### Model architecture\n",
    "\n",
    "```python\n",
    "        input = Input(shape=(80,350)\n",
    "        mask = Masking(mask_value=0.)(input)\n",
    "        hidden_layer_1 = Bidirectional(LSTM(200, return_sequences=True, activation='tahn', recurrent_activation='hard_sigmoid))(mask)  \n",
    "        model_last_layer = TimeDistributed(Dense(50, activation='relu'))(hidden_layer_1)  \n",
    "        crf = CRF(11)  \n",
    "        out = crf(model_last_layer)  \n",
    "        model_final = Model(input, out)\n",
    "        model_final.compile('RMSprop', loss=crf_loss, metrics=[crf_accuracy])\n",
    "```\n",
    "\n",
    "**Layer (Type)** | **Output Shape** | **Number of Params**\n",
    "------------ | ------------- | ------------\n",
    "input_1 (InputLayer) | (None, 80, 350) | 0\n",
    "masking_1 (Masking)  | (None, 80, 350) | 0\n",
    "bidirectional_1 (Bidirectional) | (None, 80, 400) | 601200\n",
    "time_distributed_1 (TimeDistributed) | (None, 80, 50) | 15050\n",
    "crf_1 (CRF) | (None, 80, 11) | 704\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2db2cba6a0d878e13932fa27ce6f3fb71ad99cf1)\n",
    "\n",
    "\n",
    "LSTMs are a recursive series of **matrix computations** through the various gates:\n",
    "* matrix multiplications\n",
    "* matrix addition of biases\n",
    "* matrix addition\n",
    "* applying different functions\n",
    "* pointwise addition & multiplication\n",
    "\n",
    "The shape of the matrices are influenced by the number of hidden units and input vector size. The shape/length of the hidden state is the number of hidden units. \n",
    "\n",
    "More LSTM units = Larger Hidden State = Learn more structural + semantic information from the sequence\n",
    "\n",
    "Each unit learns to remember different things about the sequence it sees\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Input Layer\n",
    "Instantite a keras tensor (multi-dimensional array)\n",
    "\n",
    "### Masking Layer\n",
    "\n",
    "Indicate the padding notation and notify tensorflow to ignore it\n",
    "\n",
    "### Bidirectional LSTM\n",
    "\n",
    "Hidden states computed in the forward sequence and backward sequence are concatenated which can be seen by the output shape that has a length of 400 (hidden state size is 400) when there were only 200 units.\n",
    "\n",
    "Learn from the sequence \n",
    "\n",
    "### Time Distributed Dense\n",
    "\n",
    "Recommended\n",
    "\n",
    "The time distributed layer will also be performing the following operation on every hidden state (300 units) for all 80 timesteps:\n",
    "\n",
    "`output = activation(dot(input, weights) + bias)`\n",
    "\n",
    "and will output a vector of length 50 for 80 timesteps (because it has 50 units).\n",
    "\n",
    "\n",
    "### CRF Layer\n",
    "\n",
    "Maps the input sequence to the output sequence using conditional random fields\n",
    "\n",
    "\n",
    "### Tensorflow backend\n",
    "\n",
    "Graph = A computational graph where a series of tensorflow operations are performed on the data \n",
    "\n",
    "Session = Executes the graph\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
