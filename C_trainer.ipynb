{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: Do not run the B_random_cv_mit_movie_query notebook at the same time as this notebook, as it is not recommended to have > 1 heavy tensorflow process running at the same time such as training or performing random search cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "curr_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sets_filepath = os.path.join(curr_dir,'training_set','movie_queries_training_dataset.csv').replace('\\\\','/')\n",
    "word_vectors_filepath = os.path.join(curr_dir,'word_vector','word_vector.txt').replace('\\\\','/')\n",
    "target_to_index_filepath = os.path.join(curr_dir,'index_converter','target_to_index.txt').replace('\\\\','/')\n",
    "save_weights_filepath = os.path.join(curr_dir,'model_training_weights','weights.{epoch:02d}.hdf5').replace('\\\\','/')\n",
    "f1_hist_filepath = os.path.join(curr_dir,'training_hist','f1_hist.txt').replace('\\\\','/')\n",
    "best_hyperparams_info_filepath = os.path.join(curr_dir,'random_search_data','best_hyperparameter_info.txt').replace('\\\\','/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.data import load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re \n",
    "from keras import backend as k\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, LSTM, Input, concatenate, TimeDistributed, Bidirectional, Masking\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy, crf_accuracy\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras.optimizers import Adam  \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, TensorBoard\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, ParameterSampler, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You only need to run the cell below once, you can delete the cell below and across all notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dictionary to convert categories to index\n",
    "with open(target_to_index_filepath, \"rb\") as t:\n",
    "    target_to_index = pickle.load(t)\n",
    "    \n",
    "f1_labels = list(target_to_index.values())\n",
    "f1_labels.pop(0)\n",
    "\n",
    "#input_sequence for sentences, output_sequence for targets of sentences\n",
    "input_sequence = []\n",
    "output_sequence = []\n",
    "\n",
    "#Save all model weights and then select the one with the best f1 score afterwards\n",
    "save_weights = ModelCheckpoint(save_weights_filepath, save_best_only=False, save_weights_only=True, monitor='loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all possible pos tags\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "all_pos = list(tagdict.keys())\n",
    "\n",
    "all_pos_tags = []\n",
    "for pos in all_pos:\n",
    "    all_pos_tags.append('pos_'+pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading pre-processed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read pre-processed dataset for training\n",
    "df = pd.read_csv(training_sets_filepath)\n",
    "df_target = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract list of tokenized words from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of words from dataframe\n",
    "tokenized_text = df['word'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectorization: FastText\n",
    "\n",
    "Need to represent words as numbers because machine learning models cannot read raw text\n",
    "\n",
    "\n",
    "* High ability to vectorize out-of-vocabulary words\n",
    "  * Some texts may contain words (names, terminologies) that popular pre-trained word vectorization models such as GloVe and Word2Vec cannot vectorize as these words were very probably not included in their training corpus\n",
    "  * FastText performs word embedding using character n-grams or sub words \n",
    "\n",
    "Example: n-gram = 3, the word 'matter' would be broken into <ma, mat, att, tte, ter, er>\n",
    "\n",
    "Used Pre-Trained Model, not enough data to train a FastText model\n",
    "\n",
    "Vectorized **lower text** because there are more lower case n-grams compared to n-grams with upper case letters in the FastText model's wikipedia training corpus\n",
    "\n",
    "Standardised vectors for numbers as numbers have no semantic meaning, they shouldn't have different vectors\n",
    "\n",
    "### Why make an API for word vectorization instead of including it in the notebook?\n",
    "\n",
    "Pre-Trained FastText model is 7GB and loading it takes up alot of time & memory. Loading >1 FastText model will result in a memory error in the second notebook\n",
    "\n",
    "API allows multiple notebooks to access the word vectorization\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "def containsNumbers(check):\n",
    "    return any(char.isdigit() for char in check)\n",
    "\n",
    "@app.route(\"/word_vectorization\", methods=['POST'])\n",
    "def word_vectorization():\n",
    "    word_vectors = []\n",
    "    tokenized_text_lower = request.json\n",
    "    for word in tokenized_text_lower:\n",
    "        if containsNumbers(word):\n",
    "            word_vector = fast_text_model.wv['<NUMBER>']\n",
    "            word_vectors.append(word_vector)\n",
    "            continue\n",
    "\n",
    "        word_vector = fast_text_model.wv[word]\n",
    "\n",
    "        word_vectors.append(word_vector)\n",
    "\n",
    "    with open(word_vectors_filepath, \"wb\") as t:\n",
    "        pickle.dump(word_vectors, t)\n",
    "    return Response(status = 200)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_api_data = tokenized_text\n",
    "session = requests.Session()\n",
    "session.trust_env = False\n",
    "session.post('http://127.0.0.1:5000/word_vectorization', json = word_vector_api_data) #add proxies args if needed\n",
    "\n",
    "with open(word_vectors_filepath, \"rb\") as t:\n",
    "    word_vectors = pickle.load(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features to the training dataframe\n",
    "\n",
    "For this set of text, there aren't many useful word features to help in learning. For instance, capitalisation cannot be considered for a word feature as the training data found online contains words that are all lower cased already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add word featues to dataframe\n",
    "df['word_vec'] = word_vectors\n",
    "df = pd.get_dummies(df, columns=['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix arrangement of columns\n",
    "\n",
    "We need to add all the pos columns from nltk and rearrange them in order for consistency. We need to add all pos columns, incase future words we predict on have a pos that our training text does not contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add all pos columns and rearrange in fixed order for consistency\n",
    "df_cols = list(df.columns)\n",
    "add_pos_col = [add for add in all_pos_tags if add not in df_cols]\n",
    "print(' ')\n",
    "print('Missing pos tags: {}'.format(len(add_pos_col)))\n",
    "print(' ')\n",
    "print('Need to add pos columns: {}'.format(add_pos_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for added_pos in add_pos_col:\n",
    "    df[added_pos] = 0\n",
    "\n",
    "arrange_df_cols = ['sentence_no','word','word_vec']\n",
    "for arrange_pos in all_pos_tags:\n",
    "    arrange_df_cols.append(arrange_pos)\n",
    "df = df.reindex(columns=arrange_df_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of X training dataset\n",
    "\n",
    "Create a dictionary where each key is a sentence number and each value is the feature vectors of all the words in that sentence\n",
    "\n",
    "The feature vector of a word will be its word vector, its word features and pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the sentence feature vectors. Each sentence contains a list of all its word feature vectors.\n",
    "df = df.drop(columns=['word'])\n",
    "sentence_feature_vectors = {}\n",
    "for index,row in df.iterrows():\n",
    "    sentence_number = row[0]\n",
    "    word_feature_vector = np.concatenate((row[1:]), axis = None)\n",
    "    if sentence_number in sentence_feature_vectors.keys():\n",
    "        sentence_feature_vectors[sentence_number].append(word_feature_vector)\n",
    "    else:\n",
    "        sentence_feature_vectors[sentence_number] = [word_feature_vector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now, each dictionary key is a sentence number. Each dictionary value is a list of word feature vectors of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Feature Vectors in the first sentence:')\n",
    "sentence_feature_vectors[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding and adding each sentence to the input sequence\n",
    "\n",
    "This is done because keras accepts fixed-length input to improve performance by creating tensors of fixed shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad length for sentences and append to the input_sequence \n",
    "\n",
    "#Length of the feature vector \n",
    "dummy_length = len(sentence_feature_vectors[1][0])\n",
    "\n",
    "#Iterate over the sentences, pad them and add them to the input sequence\n",
    "for sentence in sentence_feature_vectors.values():\n",
    "    while len(sentence) < 80:\n",
    "        sentence.append(np.array([0 for zero in range(dummy_length)]))\n",
    "\n",
    "    input_sequence.append(np.array(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input Sequence:')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of X training set: %s' %(x.shape,))\n",
    "print('Number of samples (sentences): {}'.format(x.shape[0]))\n",
    "print('Number of timesteps (word): {}'.format(x.shape[1]))\n",
    "print('Number of features (features for each timestep): {}'.format(x.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Y training dataset\n",
    "\n",
    "Create a dictionary where each key is a sentence number and each value is the targets of that sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the target for each word of the sentence\n",
    "targets = {}\n",
    "for index,row in df_target.iterrows():\n",
    "    sentence_number = row[1]\n",
    "    word_target = row[-1]\n",
    "    if sentence_number in targets.keys():\n",
    "        targets[sentence_number].append(word_target)\n",
    "    else:\n",
    "        targets[sentence_number] = [word_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion of names to index, padding and add each sentence to the output sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the targets to their respective index, pad length for sentences and append to output_sequence\n",
    "for sentence in targets.values():\n",
    "    sentence = [target_to_index[target] for target in sentence]\n",
    "    while len(sentence) < 80:\n",
    "        sentence.append(target_to_index['O'])\n",
    "\n",
    "    output_sequence.append(np.array(sentence))\n",
    "    \n",
    "y = np.array(output_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding of y values\n",
    "\n",
    "CRF needs the input and output sequence to be 3 Dimensional, which is why one-hot encoding is done for the y output values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output Sequence:')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of Y training set: %s' %(y.shape,))\n",
    "print('Number of samples (sentences): {}'.format(y.shape[0]))\n",
    "print('Number of timesteps (word): {}'.format(y.shape[1]))\n",
    "print('Number of targets (one-hot): {}'.format(y.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "Samples refer to the number of sentences we have\n",
    "\n",
    "Timesteps refer to the number of words in each sentence (80 because of padding)\n",
    "\n",
    "Features refer to the features of each word (word vector, capitalise etc.)\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s,y_s = shuffle(x,y,random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_s[:9286]\n",
    "x_test = x_s[9286:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_s[:9286]\n",
    "y_test = y_s[9286:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using the F1 score as the validation metric, thus I need to create my own callback function to perform this validation on the test data at the end of each epoch\n",
    "\n",
    "We also need to get the prediciton on the test set and reshape both y sets so that it would be 2D \n",
    "as sklearn's f1 evaluation only accepts 2D inputs. Just all the words and \n",
    "their corresponding targets, not split into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_shape = y_test.shape\n",
    "y_newshape = (y_shape[0]*y_shape[1], y_shape[-1])\n",
    "y_true_reshaped = np.reshape(y_test, y_newshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compute_f1_Of_Epoch(Callback):\n",
    "    def __init__(self, x_test, y_newshape, y_true_reshaped):\n",
    "        self.x_test = x_test\n",
    "        self.y_newshape = y_newshape\n",
    "        self.y_true_reshaped = y_true_reshaped\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.f1_of_epochs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.x_test) \n",
    "        y_pred_reshaped = np.reshape(y_pred, y_newshape)\n",
    "        self.f1_of_epochs.append(f1_score(y_true_reshaped, y_pred_reshaped, average = 'macro', labels=f1_labels))\n",
    "        print('The f1 score for this epoch is: {}'.format(f1_score(y_true_reshaped, y_pred_reshaped, average = 'macro', labels=f1_labels)))\n",
    "        return\n",
    "\n",
    "compute_f1_of_epoch = Compute_f1_Of_Epoch(x_test=x_test,y_newshape=y_newshape,y_true_reshaped=y_true_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting best hyperparameters of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameter_info = ['dummy',{'units_hyperparams': 100, 'recurrent_dropout_hyperparams': 0.3, 'optimizer_hyperparams': 'Adadelta', 'hidden_layers_hyperparams': 1, 'epochs_hyperparams': 250, 'dropout_hyperparams': 0.2, 'batch_size_hyperparams': 32}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(best_hyperparams_info_filepath, \"rb\") as t:\n",
    "    best_hyperparameter_info = pickle.load(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model(units=50, optimizer='Adam', hidden_layers=2, activation_td ='relu', dropout=0.1, recurrent_dropout=0.1):\n",
    "    hidden_layers_stored = {}\n",
    "    counter=1\n",
    "    input = Input(shape=(x.shape[1],x.shape[-1]))\n",
    "    mask = Masking(mask_value=0.)(input)\n",
    "    for hl in range(hidden_layers):\n",
    "        if counter==1:\n",
    "            hidden_layers_stored['hl_{}'.format(counter)] = Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))(mask)  \n",
    "        else:\n",
    "            hidden_layers_stored['hl_{}'.format(counter)] = Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))(hidden_layers_stored['hl_{}'.format(counter-1)])\n",
    "        counter+=1\n",
    "    model_last_layer = TimeDistributed(Dense(50, activation=activation_td))(hidden_layers_stored['hl_{}'.format(counter-1)])  \n",
    "    crf = CRF(25)  \n",
    "    out = crf(model_last_layer)  \n",
    "    model_final = Model(input, out)\n",
    "    model_final.compile(optimizer=optimizer, loss=crf_loss, metrics=[crf_accuracy])\n",
    "    return model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU Options are added to prevent this file from taking up all the GPU. You can remove it if this is the only file you are running \n",
    "#changing this cell to a markdown cell and running the one below instead\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.log_device_placement = True\n",
    "    session = tf.Session(config=config)\n",
    "    with session.as_default():\n",
    "        my_model = base_model(units=best_hyperparameter_info[1]['units_hyperparams'],optimizer=best_hyperparameter_info[1]['optimizer_hyperparams'],hidden_layers=best_hyperparameter_info[1]['hidden_layers_hyperparams'],dropout=best_hyperparameter_info[1]['dropout_hyperparams'],recurrent_dropout=best_hyperparameter_info[1]['recurrent_dropout_hyperparams'])\n",
    "        my_model.fit(x_s, y_s, epochs=best_hyperparameter_info[1]['epochs_hyperparams'], batch_size=best_hyperparameter_info[1]['batch_size_hyperparams'], callbacks=[compute_f1_of_epoch])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my_model = base_model(units=best_hyperparameter_info[1]['units_hyperparams'],optimizer=best_hyperparameter_info[1]['optimizer_hyperparams'],hidden_layers=best_hyperparameter_info[1]['hidden_layers_hyperparams'],dropout=best_hyperparameter_info[1]['dropout_hyperparams'],recurrent_dropout=best_hyperparameter_info[1]['recurrent_dropout_hyperparams'])\n",
    "my_model.fit(x_s, y_s, epochs=best_hyperparameter_info[1]['epochs_hyperparams'], batch_size=best_hyperparameter_info[1]['batch_size_hyperparams'], callbacks=[save_weights, compute_f1_of_epoch])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the validation f1 scores at the end of each epoch, we can see which model's hyperparameters are most optimal \n",
    "for our use case by picking out the one with the highest validation f1.\n",
    "\n",
    "We then iterate through the folder of all saved model hyperparameters and only save the one that is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_hist = list(compute_f1_of_epoch.f1_of_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = f1_hist.index(max(f1_hist))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_score = max(f1_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for i in range(1,251):\n",
    "    if i==best_epoch:\n",
    "        continue\n",
    "    else:\n",
    "        epoch_number = str(i)\n",
    "        if len(epoch_number) < 2:\n",
    "            epoch_number = epoch_number.zfill(2) \n",
    "\n",
    "        os.remove(os.path.join(curr_dir,'model_training_weights','weights.{}.hdf5').format(epoch_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f1_hist_filepath, \"wb\") as t:\n",
    "    pickle.dump(f1_hist, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Directional LSTM (Long Short Term Memory) with CRF (Conditional Random Fields)\n",
    "\n",
    "**Why?**\n",
    "Suitable for learning sequence-related data, sequence is very important in sentences. For example:\n",
    "<br>\n",
    "Give example\n",
    "<br>\n",
    "\n",
    "We can guess an entity from the context given\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Recurrent Neural Networks\n",
    "Feeds the information of previous time steps into the current time step to help with the prediction of the current time step\n",
    "\n",
    "**Problem:** Cannot learn long term dependencies i.e. words very early in the sentence cannot be used to help with the prediction of a word that occurs much later in the sentence. This is because of the vanshing gradient problem which arises because of matrice multiplications in RNN back-propagation through time.\n",
    "\n",
    "### Vanilla LSTM Concept\n",
    "\n",
    "![](https://i.stack.imgur.com/swN2l.png)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "* Hidden layer is made of hidden units \n",
    "  * Learning is done here\n",
    "  * Each hidden unit learns different things about the sequence\n",
    "  * Each hidden unit can be thought of as a chain of LSTM cells due to its recurrent nature\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "What an LSTM unit looks like:\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Parts of an LSTM cell:\n",
    "* Cell State (Black line at the top)\n",
    "  * LSTM's memory running throughout its recursive operations\n",
    "* Hidden State\n",
    "  * Filtered version of the cell state, contain information related to the prediction of the currenr time step that is useful for the next time step\n",
    "* Decision Gates (Yellow boxes)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### LSTM Decision Gates\n",
    "\n",
    "There are 3 decision gates in a LSTM cell: *Forget, Input, Output*\n",
    "\n",
    "#### Forget (Remember) Gate\n",
    "* **Decides** what past information is irrelevant for learning the sequence for future timesteps\n",
    "* **Decides** what information is still important for learning the sequence for future timesteps\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)\n",
    "\n",
    "1. Takes in prev hidden state and current input as its inputs\n",
    "2. LSTM weights are multiplied with their respective inputs\n",
    "3. Bias is added to each of the results\n",
    "5. Results are summed and put through a sigmoid function\n",
    "\n",
    "\n",
    "The sigmoid function outputs the values between 0 and 1. Values closer to 0 are deemed more negligible vice-versa.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Input (Update) Gate\n",
    "* **Decides** what information to add to the cell state that can be useful for the LSTM’s learnings for the future sequence \n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)\n",
    "\n",
    "First equation decides what to update and how much to update the candidate values. The second equation generates all potential candidate values to be added. The output of these two equations are point-wise multiplied to output the scaled values to be updated.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Cell State Operation (Forget Gate & Input Gate)\n",
    "* **Execution** of decisions made by *forget* and *input* gate\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)\n",
    "\n",
    "<br>\n",
    "Execution of decision made by forget gate:\n",
    "\n",
    "The forget gate’s output is pointwise multiplied by the cell state, this is where we execute the decision made by the forget gate and ‘forget’ insignificant information from the cell state. Cell state values multiplied by values closer to 0 are more likely to be ‘forgotten’ vice-versa\n",
    "\n",
    "<br>\n",
    "Execution of decision made by input gate:\n",
    "\n",
    "The cell state is then pointwise added with the input gate’s output to execute the update decision made by the input gate\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Output Gate\n",
    "* **Decides** what information we are going to output from the cell state that can be useful for the next time step\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Cell State Operation (Output Gate):\n",
    "* **Execution** of decision made by *output* gate\n",
    "\n",
    "The first equation decides what values to output from the cell state. The second equation pointwise multiplies the first equation (decision) by a tahn function applied to the cell state which results in the hidden state.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Bidirectional\n",
    "\n",
    "Same operations but runs the sequence backwards. \n",
    "\n",
    "Able to preserve information from past and future = context\n",
    "\n",
    "_\"You shall know a word by the company it keeps\" - John Rupert Firth_\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### CRF\n",
    "\n",
    "![](https://d2ueix13hy5h3i.cloudfront.net/wp-content/uploads/2019/06/CodeCogsEqn7.png)\n",
    "\n",
    "Calculating Probability of the most likely Y sequence of labels given the X sequence\n",
    "* f represents the feature functions\n",
    "* inner sum sums the feature functions of the words in the sentence\n",
    "* outer sum sums the feature functions of the sentences\n",
    "* exp is an expotential function and 1/Z(x) is a normalization which helps make it a probability\n",
    "\n",
    "![](https://d2ueix13hy5h3i.cloudfront.net/wp-content/uploads/2019/06/CodeCogsEqn1-5.png)\n",
    "![](https://d2ueix13hy5h3i.cloudfront.net/wp-content/uploads/2019/06/ss1-2.png)\n",
    "\n",
    "The feature function takes in the previous label and the current label as well as the current input into consideration when making a prediction\n",
    "\n",
    "<br>\n",
    "\n",
    "### Back propagation through time (Training)\n",
    "\n",
    "Process where the LSTM neural network updates all its parameters (weights and biases) to minimize the loss function. The loss function is the magnitude of errors that the LSTM neural network makes during prediction. Backpropagation through time can be thought of as the process of constantly finding the right direction in which to adjust the weights such that the local minimum of the loss function can be reached, which will make the LSTM neural network predict better.\n",
    "<br>\n",
    "The loss function we use is the CRF loss function which is a negative log-likelihood function.\n",
    "* Likelihood: Measures how well the parameters have adjusted to classify our entity correctly using the probabilities produced by the model\n",
    "* Log function: Regulate values as some of the likelihood values can be very small\n",
    "* Negative: Minimise the function negative so that we can maximise our classification performance by minimizing the function as the optimizers are made to minimize loss functions\n",
    "\n",
    "The closer our loss function to 0, the better\n",
    "\n",
    "\n",
    "### Model architecture\n",
    "\n",
    "```python\n",
    "        input = Input(shape=(80,350)\n",
    "        mask = Masking(mask_value=0.)(input)\n",
    "        hidden_layer_1 = Bidirectional(LSTM(200, return_sequences=True, activation='tahn', recurrent_activation='hard_sigmoid))(mask)  \n",
    "        model_last_layer = TimeDistributed(Dense(50, activation='relu'))(hidden_layer_1)  \n",
    "        crf = CRF(11)  \n",
    "        out = crf(model_last_layer)  \n",
    "        model_final = Model(input, out)\n",
    "        model_final.compile('RMSprop', loss=crf_loss, metrics=[crf_accuracy])\n",
    "```\n",
    "\n",
    "**Layer (Type)** | **Output Shape** | **Number of Params**\n",
    "------------ | ------------- | ------------\n",
    "input_1 (InputLayer) | (None, 80, 350) | 0\n",
    "masking_1 (Masking)  | (None, 80, 350) | 0\n",
    "bidirectional_1 (Bidirectional) | (None, 80, 400) | 601200\n",
    "time_distributed_1 (TimeDistributed) | (None, 80, 50) | 15050\n",
    "crf_1 (CRF) | (None, 80, 11) | 704\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2db2cba6a0d878e13932fa27ce6f3fb71ad99cf1)\n",
    "\n",
    "\n",
    "LSTMs are a recursive series of **matrix computations** through the various gates:\n",
    "* matrix multiplications\n",
    "* matrix addition of biases\n",
    "* matrix addition\n",
    "* applying different functions\n",
    "* pointwise addition & multiplication\n",
    "\n",
    "The shape of the matrices are influenced by the number of hidden units and input vector size. The shape/length of the hidden state is the number of hidden units. \n",
    "\n",
    "More LSTM units = Larger Hidden State = Learn more structural + semantic information from the sequence\n",
    "\n",
    "Each unit learns to remember different things about the sequence it sees\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Input Layer\n",
    "Instantite a keras tensor (multi-dimensional array)\n",
    "\n",
    "### Masking Layer\n",
    "\n",
    "Indicate the padding notation and notify tensorflow to ignore it\n",
    "\n",
    "### Bidirectional LSTM\n",
    "\n",
    "Hidden states computed in the forward sequence and backward sequence are concatenated which can be seen by the output shape that has a length of 400 (hidden state size is 400) when there were only 200 units.\n",
    "\n",
    "Learn from the sequence \n",
    "\n",
    "### Time Distributed Dense\n",
    "\n",
    "Recommended\n",
    "\n",
    "The time distributed layer will also be performing the following operation on every hidden state (300 units) for all 80 timesteps:\n",
    "\n",
    "`output = activation(dot(input, weights) + bias)`\n",
    "\n",
    "and will output a vector of length 50 for 80 timesteps (because it has 50 units).\n",
    "\n",
    "\n",
    "### CRF Layer\n",
    "\n",
    "Maps the input sequence to the output sequence using conditional random fields\n",
    "\n",
    "\n",
    "### Tensorflow backend\n",
    "\n",
    "Graph = A computational graph where a series of tensorflow operations are performed on the data \n",
    "\n",
    "Session = Executes the graph\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
