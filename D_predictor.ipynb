{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "curr_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_filepath = os.path.join(curr_dir,'word_vector','word_vector.txt').replace('\\\\','/')\n",
    "movie_queries_test_text_filepath = os.path.join(curr_dir,'test_set','movie_queries_test_text.txt').replace('\\\\','/')\n",
    "index_to_target_filepath = os.path.join(curr_dir,'index_converter','index_to_target.txt').replace('\\\\','/')\n",
    "target_to_index_filepath = os.path.join(curr_dir,'index_converter','target_to_index.txt').replace('\\\\','/')\n",
    "best_weights_filepath = os.path.join(curr_dir,'model_training_weights','weights.222.hdf5').replace('\\\\','/')\n",
    "best_hyperparams_info_filepath = os.path.join(curr_dir,'random_search_data','best_hyperparameter_info.txt').replace('\\\\','/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.data import load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re \n",
    "from keras import backend as k\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, LSTM, Input, concatenate, TimeDistributed, Bidirectional, Masking\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy, crf_accuracy\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras.optimizers import Adam  \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, ParameterSampler, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You only need to run the cell below once, you can delete the cell below and across all notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Test Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(movie_queries_test_text_filepath, \"rb\") as t:\n",
    "    test_text = pickle.load(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "* Add the functions needed for cleaning the raw text\n",
    "* Set a dictionary for the index to be converted back into the target name once prediction has been done\n",
    "* Initialize the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to remove . from numbers found using regex\n",
    "def remove_decimal(number):\n",
    "    return number.group(0).replace('.','')\n",
    "\n",
    "#Functions to remove , from numbers found using regex\n",
    "def remove_comma(number):\n",
    "    return number.group(0).replace(',','')\n",
    "\n",
    "def base_model(units=50, optimizer='Adam', hidden_layers=2, activation_td ='relu', dropout=0.1, recurrent_dropout=0.1):\n",
    "    hidden_layers_stored = {}\n",
    "    counter=1\n",
    "    input = Input(shape=(80,95))\n",
    "    mask = Masking(mask_value=0.)(input)\n",
    "    for hl in range(hidden_layers):\n",
    "        if counter==1:\n",
    "            hidden_layers_stored['hl_{}'.format(counter)] = Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))(mask)  \n",
    "        else:\n",
    "            hidden_layers_stored['hl_{}'.format(counter)] = Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))(hidden_layers_stored['hl_{}'.format(counter-1)])\n",
    "        counter+=1\n",
    "    model_last_layer = TimeDistributed(Dense(50, activation=activation_td))(hidden_layers_stored['hl_{}'.format(counter-1)])  \n",
    "    crf = CRF(25)  \n",
    "    out = crf(model_last_layer)  \n",
    "    model_final = Model(input, out)\n",
    "    model_final.compile(optimizer=optimizer, loss=crf_loss, metrics=[crf_accuracy])\n",
    "    return model_final\n",
    "\n",
    "#The following combination of hyperparameters is found to work quite well, which is why i added it in. If the random cv result\n",
    "#is to be used please comment out the next line and uncomment the 2 lines following it\n",
    "best_hyperparameter_info = ['dummy',{'units_hyperparams': 100, 'recurrent_dropout_hyperparams': 0.3, 'optimizer_hyperparams': 'Adadelta', 'hidden_layers_hyperparams': 1, 'epochs_hyperparams': 250, 'dropout_hyperparams': 0.2, 'batch_size_hyperparams': 32}]\n",
    "#with open(best_hyperparams_info_filepath, \"rb\") as t:\n",
    "    #best_hyperparameter_info = pickle.load(t)\n",
    "\n",
    "#GPU Options are added to prevent this file from taking up all the GPU. You can remove it if this is the only file you are running.\n",
    "graph1 = tf.Graph()\n",
    "with graph1.as_default():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.log_device_placement = True\n",
    "    session1 = tf.Session(config=config)\n",
    "    with session1.as_default():\n",
    "        model1 = base_model(units=best_hyperparameter_info[1]['units_hyperparams'],optimizer=best_hyperparameter_info[1]['optimizer_hyperparams'],hidden_layers=best_hyperparameter_info[1]['hidden_layers_hyperparams'],dropout=best_hyperparameter_info[1]['dropout_hyperparams'],recurrent_dropout=best_hyperparameter_info[1]['recurrent_dropout_hyperparams'])\n",
    "        model1.load_weights(best_weights_filepath)\n",
    "        model1._make_predict_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary to convert categories to index\n",
    "with open(index_to_target_filepath, \"rb\") as t:\n",
    "    index_to_targets = pickle.load(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize input_sequence\n",
    "input_sequence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all possible pos tags\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "all_pos = list(tagdict.keys())\n",
    "\n",
    "all_pos_tags = []\n",
    "for pos in all_pos:\n",
    "    all_pos_tags.append('pos_'+pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input text to be extracted\n",
    "\n",
    "Here, I am using the test text provided by the MIT website which I have scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = test_text[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text = \"Input a movie search query\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Numbers\n",
    "\n",
    "Decimal points from numbers will be removed during text cleaning as these decimal points could be mistaken as fullstops by the sentence tokenizer which would result in inaccurate sentence tokenization.\n",
    "\n",
    "The numbers list acts as a reference to replace the unformatted decimal numbers with their original format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find numbers\n",
    "text_find_numbers = text\n",
    "text_find_numbers = re.sub('[^a-zA-Z0-9.\\s]+','',text_find_numbers) \n",
    "numbers = re.findall('\\d*\\.?\\d+',text_find_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text cleaning\n",
    "#Characters of words that are not letters, numbers or punctuation are removed as this will impact sentence tokenization,\n",
    "#word tokenization and word vectorization\n",
    "text = \" \".join(text.splitlines())\n",
    "text = re.sub('[^a-zA-Z0-9.,\\s]+','',text) \n",
    "text = re.sub('\\s+', ' ', text).strip()\n",
    "text = re.sub('\\d*\\.?\\d+',remove_decimal,text)\n",
    "text = re.sub('\\d*\\,?\\d+',remove_comma,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Break paragraph into sentences, then break sentence into words and add pos tags to each word of a sentence \n",
    "sentences = sent_tokenize(text)\n",
    "sent_num = 0\n",
    "pos_dict = {}\n",
    "for sentence in sentences:\n",
    "    sent_num += 1\n",
    "    pos_dict[sent_num] = nltk.pos_tag(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stray . , from words. If the word value of the tuple only contains . , then remove it\n",
    "for key,value in pos_dict.items():\n",
    "    cleaned = []\n",
    "    for pos_tuple in value:\n",
    "        word_value, tag_value = pos_tuple\n",
    "        checked = re.sub('[^a-zA-Z0-9]+','',word_value) \n",
    "        if len(checked)!=len(word_value):\n",
    "            print('Found stray punctuation')\n",
    "            print('Original: {}'.format(word_value))\n",
    "            print('New: {}'.format(checked))\n",
    "            print('')\n",
    "            print('#################')\n",
    "        if len(checked) == 0 :\n",
    "            continue\n",
    "        else:\n",
    "            pos_tuple = tuple([checked, tag_value])\n",
    "            cleaned.append(pos_tuple)\n",
    "\n",
    "    pos_dict[key] = cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe with corresponding sentence number, word, and part of speech columns\n",
    "L = [(k, *t) for k, v in pos_dict.items() for t in v]\n",
    "df = pd.DataFrame(L, columns=['sentence_no','word','pos'])\n",
    "\n",
    "#Use later on\n",
    "df_for_prediction = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of words from dataframe\n",
    "tokenized_text = df['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Vectorization API\n",
    "word_vector_api_data = tokenized_text\n",
    "session = requests.Session()\n",
    "session.trust_env = False\n",
    "session.post('http://127.0.0.1:5000/word_vectorization', json = word_vector_api_data) #add proxies args if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(word_vectors_filepath, \"rb\") as t:\n",
    "    word_vectors = pickle.load(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add word featues to dataframe\n",
    "df['word_vec'] = word_vectors\n",
    "df = pd.get_dummies(df, columns=['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find pos columns needed to be added\n",
    "df_cols = list(df.columns)\n",
    "add_pos_col = [add for add in all_pos_tags if add not in df_cols]\n",
    "\n",
    "#Add missing pos columns\n",
    "for added_pos in add_pos_col:\n",
    "    df[added_pos] = 0\n",
    "\n",
    "#Rearrange columns in fixed order for consistency in training data set\n",
    "arrange_df_cols = ['sentence_no','word','word_vec']\n",
    "for arrange_pos in all_pos_tags:\n",
    "    arrange_df_cols.append(arrange_pos)\n",
    "df = df.reindex(columns=arrange_df_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the sentence feature vectors. Each sentence feature vector contains a list of all its word feature vectors.\n",
    "df = df.drop(columns=['word'])\n",
    "sentence_feature_vectors = {}\n",
    "for index,row in df.iterrows():\n",
    "    sentence_number = row[0]\n",
    "    word_feature_vector = np.concatenate((row[1:]), axis = None)\n",
    "    if sentence_number in sentence_feature_vectors.keys():\n",
    "        sentence_feature_vectors[sentence_number].append(word_feature_vector)\n",
    "    else:\n",
    "        sentence_feature_vectors[sentence_number] = [word_feature_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad length for sentences and append to the input_sequence \n",
    "dummy_length = len(sentence_feature_vectors[1][0])\n",
    "for sentence in sentence_feature_vectors.values():\n",
    "    while len(sentence) < 80:\n",
    "        sentence.append(np.array([0 for zero in range(dummy_length)]))\n",
    "        \n",
    "    input_sequence.append(np.array(sentence))\n",
    "\n",
    "x = np.array(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for all entities \n",
    "\n",
    "`np.argmax` is used to get the index of the tag with the highest probability in the arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WITHOUT np.argmax\n",
    "with session1.as_default():\n",
    "    result_dummy = model1.predict(x)\n",
    "\n",
    "print('Without np argmax the output sequence is a 3D output where each word contains one hot encodings: {}'.format(result_dummy.shape))\n",
    "result_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict y values using x values and convert integer y to its correct entity. This is for all the entities except for location\n",
    "with session1.as_default():\n",
    "    prediction = np.argmax(model1.predict(x), axis=-1)\n",
    "    \n",
    "print('With np argmax the output sequence is a 2D output where each word is an index: {}'.format(prediction.shape))\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the index to its respective target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the index of the prediction to its respective target tag\n",
    "predicted_tag = [[index_to_targets[i] for i in row] for row in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Now that we have the predicted tags we need to get their respective words by getting the word in the same position as the target in its array\n",
    "\n",
    "#### Create a list of lists of the words that has the same shape as predicted_tag list of lists\n",
    "\n",
    "#### Create a dictionary where each key is a sentence number and each value is the words of that sentence\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary with sentence number as keys and the values as the words of the sentence \n",
    "sentences = {}\n",
    "\n",
    "for index,row in df_for_prediction.iterrows():\n",
    "    sentence_number = row[0]\n",
    "    word = row[1]\n",
    "    if sentence_number in sentences.keys():\n",
    "        sentences[sentence_number].append(word)\n",
    "    else:\n",
    "        sentences[sentence_number] = [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Add padding to each word list as the prediction contains padding too\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sequence = []\n",
    "for sentence in sentences.values():\n",
    "    while len(sentence) < 80:\n",
    "        sentence.append('padding')\n",
    "    word_sequence.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Adding formatting back to numbers that are supposed to have decimal points\n",
    "\n",
    "However, some numbers may still be attached to numbers such as '4th' as recognized by the word tokenizer and so we should not separate it. The same goes for numbers such as '200+'\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "    re.findall('\\d*\\.?\\d+',word)\n",
    "``` \n",
    "searches for a number in the current word \n",
    "\n",
    "```python\n",
    "    re.search('[a-zA-Z+]', word)\n",
    "```\n",
    "searches for text and plus sign in the word. \n",
    "\n",
    "If both text and numbers are present we do not replace anything\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outer loop loops over sentences\n",
    "#Inner loop loops over words in the sentence\n",
    "\n",
    "#Add decimal points back to numbers that have them\n",
    "counter = 0\n",
    "for sentence in word_sequence:\n",
    "    curr_index = 0\n",
    "    for word in sentence:\n",
    "        \n",
    "        #Check if all the formatted numbers have been iterated through\n",
    "        if counter < len(numbers):\n",
    "            \n",
    "            #If a number is found and it is equal to the number in \n",
    "            #the numbers list with the decimal point removed\n",
    "            if re.findall('\\d*\\.?\\d+',word) == [numbers[counter].replace('.','')]:\n",
    "                \n",
    "                #If a word or plus is found in the number do not replace anything, \n",
    "                #move on to the next number\n",
    "                if re.search('[a-zA-Z+]', word):\n",
    "                    counter+=1\n",
    "                else:\n",
    "                    \n",
    "                    #replace the number with its correct formatting\n",
    "                    sentence.pop(curr_index)\n",
    "                    sentence.insert(curr_index, numbers[counter])\n",
    "                    counter += 1\n",
    "        curr_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize an empty dictionary to store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary to convert categories to index\n",
    "with open(target_to_index_filepath, \"rb\") as t:\n",
    "    old_result_dict = pickle.load(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in old_result_dict.items():\n",
    "    old_result_dict[k]=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize result dictionary\n",
    "old_result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the respective words of the targets and storing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outer loop loops through sentences, inner loop loops through the sentence\n",
    "\n",
    "Getting the corresponding word of the target now that both arrays have the exact same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outer loop loops through sentences, Inner loop loops through the sentence\n",
    "\n",
    "#set sentence counter to 0\n",
    "sent_counter = 0\n",
    "\n",
    "#iterating over sentences\n",
    "for sentence_prediction in predicted_tag:\n",
    "    \n",
    "    #set word counter to 0\n",
    "    word_counter = 0\n",
    "    for single_prediction in sentence_prediction:\n",
    "        #if the target is not O\n",
    "        if single_prediction != 'O':\n",
    "            \n",
    "            print('Predicted tag found: {}'.format(single_prediction))\n",
    "            print('Its position in the word sequence is')\n",
    "            print('    Sentence Number: {} , Index: {}'.format(sent_counter,word_counter))\n",
    "            \n",
    "            #Add the corresponding word of the predicted label to the old_result_dict using the correct subsets\n",
    "            old_result_dict[single_prediction].append(word_sequence[sent_counter][word_counter])\n",
    "            \n",
    "            print('Added word: {} from word sequence using subsets {}, {}'.format(word_sequence[sent_counter][word_counter],sent_counter,word_counter))\n",
    "            print(' ')\n",
    "        \n",
    "        word_counter+=1\n",
    "    sent_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each label has B-, I- tags, we need to consolidate the variations of the labels to its root label. i.e B-ACTOR and I-ACTOR should be considered as ACTOR.\n",
    "\n",
    "We iterate through the dictionary and extract out the root label, and add the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "for k,v in old_result_dict.items():\n",
    "    if k!='O':\n",
    "        new_key = k.split('-')[1]\n",
    "    else:\n",
    "        new_key = 'O'\n",
    "    if new_key not in result_dict.keys():\n",
    "        result_dict[new_key] = v\n",
    "    else:\n",
    "        result_dict[new_key].extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now join all the values in the arrays to make it cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_clean = {}\n",
    "for k,v in result_dict.items():\n",
    "    result_dict_clean[k] = \" \".join(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame.from_dict(result_dict_clean, orient='index')\n",
    "result_df = result_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.replace(to_replace=[None], value='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
