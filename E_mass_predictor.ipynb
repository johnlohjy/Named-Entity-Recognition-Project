{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "curr_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_filepath = os.path.join(curr_dir,'word_vector','word_vector.txt').replace('\\\\','/')\n",
    "movie_queries_test_text_filepath = os.path.join(curr_dir,'test_set','movie_queries_test_text.txt').replace('\\\\','/')\n",
    "index_to_target_filepath = os.path.join(curr_dir,'index_converter','index_to_target.txt').replace('\\\\','/')\n",
    "target_to_index_filepath = os.path.join(curr_dir,'index_converter','target_to_index.txt').replace('\\\\','/')\n",
    "best_weights_filepath = os.path.join(curr_dir,'model_training_weights','weights.best.hdf5').replace('\\\\','/')\n",
    "mass_predictor_results_filepath = os.path.join(curr_dir,'mass_predictor_results','predicted_result_{}.csv').replace('\\\\','/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.data import load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re \n",
    "from keras import backend as k\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, LSTM, Input, concatenate, TimeDistributed, Bidirectional, Masking\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy, crf_accuracy\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras.optimizers import Adam  \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, ParameterSampler, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You only need to run the cell below once, you can delete the cell below and across all notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to create base model dynamically\n",
    "#I used a dictionary and formatting to add hidden layers dynamically\n",
    "def base_model(units=50, optimizer='Adam', hidden_layers=2, activation_td ='relu', dropout=0.1, recurrent_dropout=0.1):\n",
    "    hidden_layers_stored = {}\n",
    "    counter=1\n",
    "    input = Input(shape=(80,95))\n",
    "    mask = Masking(mask_value=0.)(input)\n",
    "    for hl in range(hidden_layers):\n",
    "        if counter==1:\n",
    "            hidden_layers_stored['hl_{}'.format(counter)] = Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))(mask)  \n",
    "        else:\n",
    "            hidden_layers_stored['hl_{}'.format(counter)] = Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))(hidden_layers_stored['hl_{}'.format(counter-1)])\n",
    "        counter+=1\n",
    "    model_last_layer = TimeDistributed(Dense(50, activation=activation_td))(hidden_layers_stored['hl_{}'.format(counter-1)])  \n",
    "    crf = CRF(25)  \n",
    "    out = crf(model_last_layer)  \n",
    "    model_final = Model(input, out)\n",
    "    model_final.compile(optimizer=optimizer, loss=crf_loss, metrics=[crf_accuracy])\n",
    "    return model_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting best hyperparameters of model\n",
    "\n",
    "Change accordingly if needed. The following combination of hyperparameters is found to work quite well, which is why i added it in. If you wish to use the best hyperparameter info from your own random search cv, please change the cell below this cell to a markdown cell and the one below that to a code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameter_info = ['dummy',{'units_hyperparams': 100, 'recurrent_dropout_hyperparams': 0.3, 'optimizer_hyperparams': 'Adadelta', 'hidden_layers_hyperparams': 1, 'epochs_hyperparams': 250, 'dropout_hyperparams': 0.2, 'batch_size_hyperparams': 32}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(best_hyperparams_info_filepath, \"rb\") as t:\n",
    "    best_hyperparameter_info = pickle.load(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing predictive model - graph, session, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#GPU Options are added to prevent the program from taking up all the computer GPU's memory when initializing the model\n",
    "#for prediction\n",
    "graph_masspredictor = tf.Graph()\n",
    "with graph_masspredictor.as_default():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session_masspredictor = tf.Session(config=config)\n",
    "    with session_masspredictor.as_default():\n",
    "        model_masspredictor = base_model(units=best_hyperparameter_info[1]['units_hyperparams'],optimizer=best_hyperparameter_info[1]['optimizer_hyperparams'],hidden_layers=best_hyperparameter_info[1]['hidden_layers_hyperparams'],dropout=best_hyperparameter_info[1]['dropout_hyperparams'],recurrent_dropout=best_hyperparameter_info[1]['recurrent_dropout_hyperparams'])\n",
    "        model_masspredictor.load_weights(best_weights_filepath)\n",
    "        model_masspredictor._make_predict_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to remove . and , from numbers found using regex\n",
    "def remove_decimal(number):\n",
    "    return number.group(0).replace('.','')\n",
    "\n",
    "def remove_comma(number):\n",
    "    return number.group(0).replace(',','')\n",
    "\n",
    "#Dictionary to convert index to categories\n",
    "with open(index_to_target_filepath, \"rb\") as t:\n",
    "    index_to_targets = pickle.load(t)\n",
    "\n",
    "#Dictionary to hold all results\n",
    "results = {}\n",
    "\n",
    "results_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all possible pos tags\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "all_pos = list(tagdict.keys())\n",
    "\n",
    "all_pos_tags = []\n",
    "for pos in all_pos:\n",
    "    all_pos_tags.append('pos_'+pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(movie_queries_test_text_filepath, \"rb\") as t:\n",
    "    test_text = pickle.load(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input texts to be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test_text[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_text = #make sure this is a list of movie search queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for text in test_text:\n",
    "    \n",
    "    #Initialize input_sequence\n",
    "    input_sequence = []\n",
    "    \n",
    "    #Find numbers\n",
    "    text_find_numbers = text\n",
    "    text_find_numbers = re.sub('[^a-zA-Z0-9.\\s]+','',text_find_numbers) \n",
    "    numbers = re.findall('\\d*\\.?\\d+',text_find_numbers)\n",
    "\n",
    "    #Text pre-processing\n",
    "    text = \" \".join(text.splitlines())\n",
    "    text = re.sub('[^a-zA-Z0-9.,\\s+]+','',text) \n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    text = re.sub('\\d*\\.?\\d+',remove_decimal,text)\n",
    "    text = re.sub('\\d*\\,?\\d+',remove_comma,text)\n",
    "    \n",
    "    #Break text into sentences then break sentence into words and add pos tags to each word of the sentence\n",
    "    sentences = sent_tokenize(text)\n",
    "    sent_num = 0\n",
    "    pos_dict = {}\n",
    "    for sentence in sentences:\n",
    "        sent_num += 1\n",
    "        pos_dict[sent_num] = nltk.pos_tag(word_tokenize(sentence))\n",
    "    \n",
    "    #Remove stray . , from words. If the word value of the tuple only contains . , then remove it\n",
    "    for key,value in pos_dict.items():\n",
    "        cleaned = []\n",
    "        for pos_tuple in value:\n",
    "            word_value, tag_value = pos_tuple\n",
    "            checked = re.sub('[^a-zA-Z0-9]+','',word_value) \n",
    "            if len(checked) == 0 :\n",
    "                continue\n",
    "            else:\n",
    "                pos_tuple = tuple([checked, tag_value])\n",
    "                cleaned.append(pos_tuple)\n",
    "\n",
    "        pos_dict[key] = cleaned\n",
    "\n",
    "    #Create dataframe with corresponding sentence number, word, and part of speech columns\n",
    "    L = [(k, *t) for k, v in pos_dict.items() for t in v]\n",
    "    df = pd.DataFrame(L, columns=['sentence_no','word','pos'])\n",
    "    \n",
    "    #Use later on\n",
    "    df_for_prediction = df.copy()\n",
    "\n",
    "    #Get list of words from dataframe. Create another list where words are all lower cased.\n",
    "    tokenized_text = df['word'].tolist()\n",
    "\n",
    "    word_vector_api_data = tokenized_text\n",
    "    session = requests.Session()\n",
    "    session.trust_env = False\n",
    "    session.post('http://127.0.0.1:5000/word_vectorization', json = word_vector_api_data) #add proxies args if needed\n",
    "    \n",
    "    with open(word_vectors_filepath, \"rb\") as t:\n",
    "        word_vectors = pickle.load(t)\n",
    "    \n",
    "    #Add word featues to dataframe\n",
    "    df['word_vec'] = word_vectors\n",
    "    df = pd.get_dummies(df, columns=['pos'])\n",
    "\n",
    "    #Add all pos columns and rearrange in fixed order for consistency\n",
    "    df_cols = list(df.columns)\n",
    "    add_pos_col = [add for add in all_pos_tags if add not in df_cols]\n",
    "\n",
    "    for added_pos in add_pos_col:\n",
    "        df[added_pos] = 0\n",
    "\n",
    "    arrange_df_cols = ['sentence_no','word','word_vec']\n",
    "    for arrange_pos in all_pos_tags:\n",
    "        arrange_df_cols.append(arrange_pos)\n",
    "    df = df.reindex(columns=arrange_df_cols)\n",
    "\n",
    "    #Get the sentence feature vectors. Each sentence contains a list of all its word feature vectors.\n",
    "    df = df.drop(columns=['word'])\n",
    "    sentence_feature_vectors = {}\n",
    "    for index,row in df.iterrows():\n",
    "        sentence_number = row[0]\n",
    "        word_feature_vector = np.concatenate((row[1:]), axis = None)\n",
    "        if sentence_number in sentence_feature_vectors.keys():\n",
    "            sentence_feature_vectors[sentence_number].append(word_feature_vector)\n",
    "        else:\n",
    "            sentence_feature_vectors[sentence_number] = [word_feature_vector]\n",
    "   \n",
    "    #Pad length for sentences and append to the input_sequence \n",
    "    dummy_length = len(sentence_feature_vectors[1][0])\n",
    "    for sentence in sentence_feature_vectors.values():\n",
    "        while len(sentence) < 80:\n",
    "            sentence.append(np.array([0 for zero in range(dummy_length)]))\n",
    "            \n",
    "        input_sequence.append(np.array(sentence))\n",
    "        \n",
    "    x = np.array(input_sequence)\n",
    "    \n",
    "    \n",
    "    #Predict y values using x values and convert integer y to its correct entity\n",
    "    with session_masspredictor.as_default():\n",
    "        prediction = np.argmax(model_masspredictor.predict(x), axis=-1)\n",
    "    predicted_tag = [[index_to_targets[i] for i in row] for row in prediction]\n",
    "    \n",
    "    #Generate a padded word sequence for each sentence\n",
    "    sentences = {}\n",
    "    word_sequence = []\n",
    "    \n",
    "    for index,row in df_for_prediction.iterrows():\n",
    "        sentence_number = row[0]\n",
    "        word = row[1]\n",
    "        if sentence_number in sentences.keys():\n",
    "            sentences[sentence_number].append(word)\n",
    "        else:\n",
    "            sentences[sentence_number] = [word]\n",
    "\n",
    "    for sentence in sentences.values():\n",
    "        while len(sentence) < 80:\n",
    "            sentence.append('padding')\n",
    "        word_sequence.append(sentence)\n",
    "    \n",
    "    #Add decimal points back to numbers that have them\n",
    "    counter = 0\n",
    "    for sentence in word_sequence:\n",
    "        curr_index = 0\n",
    "        for word in sentence:\n",
    "            if counter < len(numbers):\n",
    "                if re.findall('\\d*\\.?\\d+',word) == [numbers[counter].replace('.','')]:\n",
    "                    if re.search('[a-zA-Z+]', word):\n",
    "                        counter+=1\n",
    "                    else:\n",
    "                        sentence.pop(curr_index)\n",
    "                        sentence.insert(curr_index, numbers[counter])\n",
    "                        counter += 1\n",
    "            curr_index += 1\n",
    "    \n",
    "    \n",
    "    with open(target_to_index_filepath, \"rb\") as t:\n",
    "        old_result_dict = pickle.load(t)\n",
    "        \n",
    "    for k,v in old_result_dict.items():\n",
    "        old_result_dict[k]=[]\n",
    "    \n",
    "    #set sentence counter to 0\n",
    "    sent_counter = 0\n",
    "    for sentence_prediction in predicted_tag:\n",
    "        word_counter = 0\n",
    "        for single_prediction in sentence_prediction:\n",
    "            if single_prediction != 'O':\n",
    "                old_result_dict[single_prediction].append(word_sequence[sent_counter][word_counter])\n",
    "            word_counter+=1\n",
    "        sent_counter+=1\n",
    "        \n",
    "    result_dict = {}\n",
    "    for k,v in old_result_dict.items():\n",
    "        if k!='O':\n",
    "            new_key = k.split('-')[1]\n",
    "        else:\n",
    "            new_key = 'O'\n",
    "        if new_key not in result_dict.keys():\n",
    "            result_dict[new_key] = v\n",
    "        else:\n",
    "            result_dict[new_key].extend(v)\n",
    "            \n",
    "    result_dict_clean = {}\n",
    "    for k,v in result_dict.items():\n",
    "        result_dict_clean[k] = \" \".join(v)\n",
    "    \n",
    "    result_df = pd.DataFrame.from_dict(result_dict_clean, orient='index')\n",
    "    result_df = result_df.transpose()\n",
    "    results['df_{}'.format(results_counter)] = result_df.replace(to_replace=[None], value='')\n",
    "    results_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    results['df_{}'.format(i)].to_csv(mass_predictor_results_filepath.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Test Text Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are there any good romantic comedies out right now'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O</th>\n",
       "      <th>ACTOR</th>\n",
       "      <th>CHARACTER</th>\n",
       "      <th>DIRECTOR</th>\n",
       "      <th>GENRE</th>\n",
       "      <th>PLOT</th>\n",
       "      <th>RATING</th>\n",
       "      <th>RATINGS_AVERAGE</th>\n",
       "      <th>REVIEW</th>\n",
       "      <th>SONG</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>TRAILER</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>romantic comedies</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  O ACTOR CHARACTER DIRECTOR              GENRE PLOT RATING RATINGS_AVERAGE  \\\n",
       "0                             romantic comedies                               \n",
       "\n",
       "  REVIEW SONG TITLE TRAILER YEAR  \n",
       "0                                 "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['df_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'show me a movie about cars that talk'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O</th>\n",
       "      <th>ACTOR</th>\n",
       "      <th>CHARACTER</th>\n",
       "      <th>DIRECTOR</th>\n",
       "      <th>GENRE</th>\n",
       "      <th>PLOT</th>\n",
       "      <th>RATING</th>\n",
       "      <th>RATINGS_AVERAGE</th>\n",
       "      <th>REVIEW</th>\n",
       "      <th>SONG</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>TRAILER</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>cars</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  O ACTOR CHARACTER DIRECTOR GENRE  PLOT RATING RATINGS_AVERAGE REVIEW SONG  \\\n",
       "0                                   cars                                      \n",
       "\n",
       "  TITLE TRAILER YEAR  \n",
       "0                     "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['df_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'list the five star rated movies starring mel gibson'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O</th>\n",
       "      <th>ACTOR</th>\n",
       "      <th>CHARACTER</th>\n",
       "      <th>DIRECTOR</th>\n",
       "      <th>GENRE</th>\n",
       "      <th>PLOT</th>\n",
       "      <th>RATING</th>\n",
       "      <th>RATINGS_AVERAGE</th>\n",
       "      <th>REVIEW</th>\n",
       "      <th>SONG</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>TRAILER</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>mel gibson</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>five</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  O       ACTOR CHARACTER DIRECTOR GENRE PLOT RATING RATINGS_AVERAGE REVIEW  \\\n",
       "0    mel gibson                                                 five          \n",
       "\n",
       "  SONG TITLE TRAILER YEAR  \n",
       "0                          "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['df_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
