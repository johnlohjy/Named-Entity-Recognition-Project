{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "curr_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_filepath = os.path.join(curr_dir,'word_vector','word_vector.txt').replace('\\\\','/')\n",
    "movie_queries_test_text_filepath = os.path.join(curr_dir,'test_set','movie_queries_test_text.txt').replace('\\\\','/')\n",
    "index_to_target_filepath = os.path.join(curr_dir,'index_converter','index_to_target.txt').replace('\\\\','/')\n",
    "target_to_index_filepath = os.path.join(curr_dir,'index_converter','target_to_index.txt').replace('\\\\','/')\n",
    "best_weights_filepath = os.path.join(curr_dir,'model_training_weights','weights.222.hdf5').replace('\\\\','/')\n",
    "mass_predictor_results_filepath = os.path.join(curr_dir,'mass_predictor_results','predicted_result_{}.csv').replace('\\\\','/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.data import load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re \n",
    "from keras import backend as k\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, LSTM, Input, concatenate, TimeDistributed, Bidirectional, Masking\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy, crf_accuracy\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras.optimizers import Adam  \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, ParameterSampler, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You only need to run the cell below once, you can delete the cell below and across all notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create model\n",
    "def base_model(units=50, optimizer='Adam', hidden_layers=2, activation_td ='relu', dropout=0.1, recurrent_dropout=0.1):\n",
    "    hidden_layers_stored = {}\n",
    "    counter=1\n",
    "    input = Input(shape=(80,95))\n",
    "    mask = Masking(mask_value=0.)(input)\n",
    "    for hl in range(hidden_layers):\n",
    "        if counter==1:\n",
    "            hidden_layers_stored['hl_{}'.format(counter)] = Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))(mask)  \n",
    "        else:\n",
    "            hidden_layers_stored['hl_{}'.format(counter)] = Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))(hidden_layers_stored['hl_{}'.format(counter-1)])\n",
    "        counter+=1\n",
    "    model_last_layer = TimeDistributed(Dense(50, activation=activation_td))(hidden_layers_stored['hl_{}'.format(counter-1)])  \n",
    "    crf = CRF(25)  \n",
    "    out = crf(model_last_layer)  \n",
    "    model_final = Model(input, out)\n",
    "    model_final.compile(optimizer=optimizer, loss=crf_loss, metrics=[crf_accuracy])\n",
    "    return model_final\n",
    "\n",
    "#The following combination of hyperparameters is found to work quite well, which is why i added it in. If the random cv result\n",
    "#is to be used please comment out the next line and uncomment the 2 lines following it\n",
    "best_hyperparameter_info = ['dummy',{'units_hyperparams': 100, 'recurrent_dropout_hyperparams': 0.3, 'optimizer_hyperparams': 'Adadelta', 'hidden_layers_hyperparams': 1, 'epochs_hyperparams': 250, 'dropout_hyperparams': 0.2, 'batch_size_hyperparams': 32}]\n",
    "#with open(best_hyperparams_info_filepath, \"rb\") as t:\n",
    "    #best_hyperparameter_info = pickle.load(t)\n",
    "    \n",
    "#GPU Options are added to prevent this file from taking up all the GPU. You can remove it if this is the only file you are running.\n",
    "graph3 = tf.Graph()\n",
    "with graph3.as_default():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.log_device_placement = True\n",
    "    session3 = tf.Session(config=config)\n",
    "    with session3.as_default():\n",
    "        model3 = base_model(units=best_hyperparameter_info[1]['units_hyperparams'],optimizer=best_hyperparameter_info[1]['optimizer_hyperparams'],hidden_layers=best_hyperparameter_info[1]['hidden_layers_hyperparams'],dropout=best_hyperparameter_info[1]['dropout_hyperparams'],recurrent_dropout=best_hyperparameter_info[1]['recurrent_dropout_hyperparams'])\n",
    "        model3.load_weights(best_weights_filepath)\n",
    "        model3._make_predict_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to remove . and , from numbers found using regex\n",
    "def remove_decimal(number):\n",
    "    return number.group(0).replace('.','')\n",
    "\n",
    "def remove_comma(number):\n",
    "    return number.group(0).replace(',','')\n",
    "\n",
    "#Dictionary to convert index to categories\n",
    "with open(index_to_target_filepath, \"rb\") as t:\n",
    "    index_to_targets = pickle.load(t)\n",
    "\n",
    "#Dictionary to hold all results\n",
    "results = {}\n",
    "\n",
    "results_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all possible pos tags\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "all_pos = list(tagdict.keys())\n",
    "\n",
    "all_pos_tags = []\n",
    "for pos in all_pos:\n",
    "    all_pos_tags.append('pos_'+pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(movie_queries_test_text_filepath, \"rb\") as t:\n",
    "    test_text = pickle.load(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input texts to be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test_text[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_text = #make sure this is a list of movie search queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for text in test_text:\n",
    "    \n",
    "    #Initialize input_sequence\n",
    "    input_sequence = []\n",
    "    \n",
    "    #Find numbers\n",
    "    text_find_numbers = text\n",
    "    text_find_numbers = re.sub('[^a-zA-Z0-9.\\s]+','',text_find_numbers) \n",
    "    numbers = re.findall('\\d*\\.?\\d+',text_find_numbers)\n",
    "\n",
    "    #Text pre-processing\n",
    "    text = \" \".join(text.splitlines())\n",
    "    text = re.sub('[^a-zA-Z0-9.,\\s+]+','',text) \n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    text = re.sub('\\d*\\.?\\d+',remove_decimal,text)\n",
    "    text = re.sub('\\d*\\,?\\d+',remove_comma,text)\n",
    "    \n",
    "    #Break text into sentences then break sentence into words and add pos tags to each word of the sentence\n",
    "    sentences = sent_tokenize(text)\n",
    "    sent_num = 0\n",
    "    pos_dict = {}\n",
    "    for sentence in sentences:\n",
    "        sent_num += 1\n",
    "        pos_dict[sent_num] = nltk.pos_tag(word_tokenize(sentence))\n",
    "    \n",
    "    #Remove stray . , from words. If the word value of the tuple only contains . , then remove it\n",
    "    for key,value in pos_dict.items():\n",
    "        cleaned = []\n",
    "        for pos_tuple in value:\n",
    "            word_value, tag_value = pos_tuple\n",
    "            checked = re.sub('[^a-zA-Z0-9]+','',word_value) \n",
    "            if len(checked) == 0 :\n",
    "                continue\n",
    "            else:\n",
    "                pos_tuple = tuple([checked, tag_value])\n",
    "                cleaned.append(pos_tuple)\n",
    "\n",
    "        pos_dict[key] = cleaned\n",
    "\n",
    "    #Create dataframe with corresponding sentence number, word, and part of speech columns\n",
    "    L = [(k, *t) for k, v in pos_dict.items() for t in v]\n",
    "    df = pd.DataFrame(L, columns=['sentence_no','word','pos'])\n",
    "    \n",
    "    #Use later on\n",
    "    df_for_prediction = df.copy()\n",
    "\n",
    "    #Get list of words from dataframe. Create another list where words are all lower cased.\n",
    "    tokenized_text = df['word'].tolist()\n",
    "\n",
    "    word_vector_api_data = tokenized_text\n",
    "    session = requests.Session()\n",
    "    session.trust_env = False\n",
    "    session.post('http://127.0.0.1:5000/word_vectorization', json = word_vector_api_data) #add proxies args if needed\n",
    "    \n",
    "    with open(word_vectors_filepath, \"rb\") as t:\n",
    "        word_vectors = pickle.load(t)\n",
    "    \n",
    "    #Add word featues to dataframe\n",
    "    df['word_vec'] = word_vectors\n",
    "    df = pd.get_dummies(df, columns=['pos'])\n",
    "\n",
    "    #Add all pos columns and rearrange in fixed order for consistency\n",
    "    df_cols = list(df.columns)\n",
    "    add_pos_col = [add for add in all_pos_tags if add not in df_cols]\n",
    "\n",
    "    for added_pos in add_pos_col:\n",
    "        df[added_pos] = 0\n",
    "\n",
    "    arrange_df_cols = ['sentence_no','word','word_vec']\n",
    "    for arrange_pos in all_pos_tags:\n",
    "        arrange_df_cols.append(arrange_pos)\n",
    "    df = df.reindex(columns=arrange_df_cols)\n",
    "\n",
    "    #Get the sentence feature vectors. Each sentence contains a list of all its word feature vectors.\n",
    "    df = df.drop(columns=['word'])\n",
    "    sentence_feature_vectors = {}\n",
    "    for index,row in df.iterrows():\n",
    "        sentence_number = row[0]\n",
    "        word_feature_vector = np.concatenate((row[1:]), axis = None)\n",
    "        if sentence_number in sentence_feature_vectors.keys():\n",
    "            sentence_feature_vectors[sentence_number].append(word_feature_vector)\n",
    "        else:\n",
    "            sentence_feature_vectors[sentence_number] = [word_feature_vector]\n",
    "   \n",
    "    #Pad length for sentences and append to the input_sequence \n",
    "    dummy_length = len(sentence_feature_vectors[1][0])\n",
    "    for sentence in sentence_feature_vectors.values():\n",
    "        while len(sentence) < 80:\n",
    "            sentence.append(np.array([0 for zero in range(dummy_length)]))\n",
    "            \n",
    "        input_sequence.append(np.array(sentence))\n",
    "        \n",
    "    x = np.array(input_sequence)\n",
    "    \n",
    "    \n",
    "    #Predict y values using x values and convert integer y to its correct entity\n",
    "    with session3.as_default():\n",
    "        prediction = np.argmax(model3.predict(x), axis=-1)\n",
    "    predicted_tag = [[index_to_targets[i] for i in row] for row in prediction]\n",
    "    \n",
    "    #Generate a padded word sequence for each sentence\n",
    "    sentences = {}\n",
    "    word_sequence = []\n",
    "    \n",
    "    for index,row in df_for_prediction.iterrows():\n",
    "        sentence_number = row[0]\n",
    "        word = row[1]\n",
    "        if sentence_number in sentences.keys():\n",
    "            sentences[sentence_number].append(word)\n",
    "        else:\n",
    "            sentences[sentence_number] = [word]\n",
    "\n",
    "    for sentence in sentences.values():\n",
    "        while len(sentence) < 80:\n",
    "            sentence.append('padding')\n",
    "        word_sequence.append(sentence)\n",
    "    \n",
    "    #Add decimal points back to numbers that have them\n",
    "    counter = 0\n",
    "    for sentence in word_sequence:\n",
    "        curr_index = 0\n",
    "        for word in sentence:\n",
    "            if counter < len(numbers):\n",
    "                if re.findall('\\d*\\.?\\d+',word) == [numbers[counter].replace('.','')]:\n",
    "                    if re.search('[a-zA-Z+]', word):\n",
    "                        counter+=1\n",
    "                    else:\n",
    "                        sentence.pop(curr_index)\n",
    "                        sentence.insert(curr_index, numbers[counter])\n",
    "                        counter += 1\n",
    "            curr_index += 1\n",
    "    \n",
    "    \n",
    "    with open(target_to_index_filepath, \"rb\") as t:\n",
    "        old_result_dict = pickle.load(t)\n",
    "        \n",
    "    for k,v in old_result_dict.items():\n",
    "        old_result_dict[k]=[]\n",
    "    \n",
    "    #set sentence counter to 0\n",
    "    sent_counter = 0\n",
    "    for sentence_prediction in predicted_tag:\n",
    "        word_counter = 0\n",
    "        for single_prediction in sentence_prediction:\n",
    "            if single_prediction != 'O':\n",
    "                old_result_dict[single_prediction].append(word_sequence[sent_counter][word_counter])\n",
    "            word_counter+=1\n",
    "        sent_counter+=1\n",
    "        \n",
    "    result_dict = {}\n",
    "    for k,v in old_result_dict.items():\n",
    "        if k!='O':\n",
    "            new_key = k.split('-')[1]\n",
    "        else:\n",
    "            new_key = 'O'\n",
    "        if new_key not in result_dict.keys():\n",
    "            result_dict[new_key] = v\n",
    "        else:\n",
    "            result_dict[new_key].extend(v)\n",
    "            \n",
    "    result_dict_clean = {}\n",
    "    for k,v in result_dict.items():\n",
    "        result_dict_clean[k] = \" \".join(v)\n",
    "    \n",
    "    result_df = pd.DataFrame.from_dict(result_dict_clean, orient='index')\n",
    "    result_df = result_df.transpose()\n",
    "    results['df_{}'.format(results_counter)] = result_df.replace(to_replace=[None], value='')\n",
    "    results_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    results['df_{}'.format(i)].to_csv(mass_predictor_results_filepath.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
